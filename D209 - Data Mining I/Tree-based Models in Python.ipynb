{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 1 - Classification and  Regression Tree (CART)\n",
    "- sequence of if-else questions about features\n",
    "- <b>Objective: </b>infer class labels\n",
    "- Able to capture non-linear relationships between features and labels\n",
    "- Don't require features to be on the same scale\n",
    "- A classification tree divides the feature space into rectangular regions. In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions.\n",
    "- <b>Advantages:</b>\n",
    "    - Simple to Understand\n",
    "    - Simple to Interpret\n",
    "    - Easy to use\n",
    "    - Flexibility, ability to describe non-linear dependencies\n",
    "    - No need to standardize or normalize features\n",
    "- <b>Limitations:</b>\n",
    "    - Can only produce orthogonal decision boundaries\n",
    "    - senstive to small variations in training sets\n",
    "    - high variance: unconstrained CARTs may overfit training set\n",
    " \n",
    "\n",
    "### Fitting a Classification-tree using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Accuracy = 0.93\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "from sklearn import datasets\n",
    "df = datasets.load_breast_cancer()\n",
    "X = df.data\n",
    "y = df.target\n",
    "\n",
    "# Import DecisionTreeClassifer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data 80/20 for train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "#Instantiate the decision tree (dt)\n",
    "dtree = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Fit the tree model\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Use model to predict y\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "# Test the accuracy score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Model Accuracy = {:.2f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks of a decision tree\n",
    "- <b>Decision Tree:</b> data structure consisting of a hierarchy of nodes\n",
    "- Recursivce process which maximizes the information at each node when determining the feature and split point\n",
    "- <b>Node: </b>question or prediction\n",
    "    - <b>Root: </b>no parent node, question giving rise to two children nodes\n",
    "    - <b>Internal Node: </b>one parent node, question giving rise to two children nodes\n",
    "    - <b>Leaf: </b>one parent node, no children nodes --> <b>Prediction</b>\n",
    "    \n",
    "### Decision Tree for regression\n",
    "- target is a continuous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test set RMSE of dt: 0.23\n"
     ]
    }
   ],
   "source": [
    "# Not on a valid data set. Just for process learning purposes\n",
    "\n",
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 2 - The Bias-Variance Tradeoff\n",
    "#### Difficulties in estabilshing f (Approximating the prediction function)\n",
    "- Overfitting: f fits the noise and not the actual relationship\n",
    "    - can be shown by low training set error but high test set error\n",
    "- Underfitting: f is not flexible enough to approximate the value\n",
    "    - training and testing error are similar, but are relatively high\n",
    "    \n",
    "\n",
    "#### Generalization Error\n",
    "- f-hat = bias**2 + variance + irreducible error\n",
    "    - bias: how much f-hat and f are different. High bias models lead to underfitting\n",
    "    - variance: how much f-hat is inconsistent over different training sets. high variance leads to over fitting\n",
    "    \n",
    "#### Goal is to find model with lowest generalization error\n",
    "As model bias decreases, the variance will increase. Need to find the sweet spot of model complexity\n",
    "\n",
    "## Diagnosing Bias and Variance Problems\n",
    "- first split the data into a training and test set\n",
    "- test set should only be used to evaluate final performance after the model is tuned on the training set\n",
    "\n",
    "### Cross-Validation\n",
    "- k-Fold CV: data is split into k sections, or folds. Each fold is held out of a model being fit on all other folds of the data combined as a training set. This is repeated k time, one for each fold as a test set. At the end you have a list of k error values from the process. The mean of these can be used for the models error\n",
    "- <b>High Variance</b> if the error from the CV is greater than the training set error\n",
    "    - you have overfit the training set\n",
    "    - decrease the model complexity (ex. decrease max depth or increase the min_samples_leaf\n",
    "    - gather more data\n",
    "- <b>High Bias</b> if CV error is similar to training error, but both are much higher than desired error\n",
    "    - Underfit the training set\n",
    "    - increase the model complexity (ex. increase max depth or decresae the min_samples_leaf)\n",
    "    - gather more relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b7cf03530a60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Evaluate the list of MSE obtained by 10-fold CV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Set n_jobs to -1 in order to exploit all CPU scores in computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mMSE_CV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Fit dtree to the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[0;32m    441\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    244\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    245\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 246\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    247\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    248\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python_ML\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n"
     ]
    }
   ],
   "source": [
    "# Not on appropriate data, just for process reference\n",
    "\n",
    "# Imports\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set Seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate Decision Tree Regressor\n",
    "dtree = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=SEED)\n",
    "\n",
    "# Evaluate the list of MSE obtained by 10-fold CV\n",
    "# Set n_jobs to -1 in order to exploit all CPU scores in computation\n",
    "MSE_CV = - cross_val_score(dtree, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "# Fit dtree to the training set\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of training set\n",
    "y_predict_train = dtree.predict(X_train)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_predict_test = dtree.predict(X_test)\n",
    "\n",
    "# Evaluate the MSE for each option\n",
    "print('CV MSE: {:.2f}'.format(MSE_CV.mean()))\n",
    "print('Train MSE: {:.2f}'.format(MSE(y_train, y_predict_train)))\n",
    "print('Test MSE: {:.2f}'.format(MSE(y_test, y_predict_test)))\n",
    "print('\\n')\n",
    "print('The CV error is higher than the training set which shows we have overfit the data. Try reducing the depth or increasing the min samples per leaf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "Solution to the limitations of CARTs\n",
    "- train different models on the same dataset\n",
    "- let each model make its predictions\n",
    "- meta-model: aggregates predictions of individual models\n",
    "- Final prediction is more robust and less prone to errors\n",
    "- best results when models are skillful but in different ways\n",
    "\n",
    "##### Example steps\n",
    "- training set is fed to multiple classifiers (Decision Tree, Logistic Regression, kNN, other...)\n",
    "- each model makes predictions\n",
    "- predictions are fed to a \"Meta-Model\" which aggregates the predictions and outputs a final prediction\n",
    "\n",
    "### Voting Classifier - Binary Classification Task\n",
    "Binary Predictions are fed to the meta-model and which ever prediction has the majority is the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.924\n",
      "K Nearest Neighbors: 0.930\n",
      "Classification Tree: 0.936\n",
      "Voting Classifier: 0.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16084\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\16084\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Import Metrics and splitting functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import models to use\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Import Data\n",
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Set Seed \n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate individual Classifiers\n",
    "lr = LogisticRegression()\n",
    "knn = KNN()\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# Define a list of tuples that contains the name of the model and the model itself\n",
    "classifiers = [('Logistic Regression', lr), \n",
    "               ('K Nearest Neighbors', knn),\n",
    "               ('Classification Tree', dtree)]\n",
    "\n",
    "# Iterate over the defind list of tuples containing the classifiers\n",
    "for clf_name, clf in classifiers:\n",
    "    # Fit clf to training set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels of the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the accuracy of the clf on the test set\n",
    "    print('{}: {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "# Instantiate a VotingClassifier 'vc'\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "# Fit vc to the training set and predict the test labels\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 3 - Bagging and Random Forests\n",
    "\n",
    "## Bagging (Bootstrap Aggregation)\n",
    "- Reduces variance of individual models in the ensemble\n",
    "- Ensemble is formed by models that use the same training algorithm. However, these models are not trained on the entire set. They are trained on different subsets of the data. \n",
    "- In bootstrapping, a sample is drawn with replacement. Meaning, a sample can be pulled from the dataset more than once to be used in the model training. \n",
    "- Creates many models to use. When a new data point(s) is fed to the algorithm, each model adds it's prediction to the meta-model\n",
    "- <b>Classification</b>\n",
    "    - Aggregates predictions by majority voting\n",
    "    - BaggingClassifier in sklearn\n",
    "- <b>Regression</b>\n",
    "    - Aggregates predictions through averaging\n",
    "    - BaggingRegressor in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision-tree Classifier: 0.906\n",
      "Accuracy of Bagging Classifier: 0.947\n"
     ]
    }
   ],
   "source": [
    "# Import Metrics and splitting functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import models to use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Import Data\n",
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Set Seed \n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification tree and Bagging Classifier\n",
    "dtree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "bc = BaggingClassifier(base_estimator=dtree, n_estimators=300, n_jobs=-1)\n",
    "\n",
    "# Fit and predict using baseline estimator\n",
    "dtree.fit(X_train, y_train)\n",
    "y_pred = dtree.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Decision-tree Classifier: {:.3f}'.format(acc))\n",
    "\n",
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate and print accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Bag (OOB) Evaluation\n",
    "- On average, for each model, 63% of the training instances are sampled\n",
    "- The remaining 37% constitute the OOB instances\n",
    "- These points can be used to test the model without the need for cross validation\n",
    "- the OOB score is the average of all OOB scores from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.942\n",
      "OOB accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "# Import Metrics and splitting functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import models to use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Import Data\n",
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Set Seed \n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification tree \n",
    "dtree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier; set oob_score=True\n",
    "bc = BaggingClassifier(base_estimator=dtree, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "# Fit bc to training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Extract OOB score from bc\n",
    "oob_acc = bc.oob_score_\n",
    "\n",
    "# Print the accuracy scores and compare\n",
    "print('Test set accuracy: {:.3f}'.format(test_acc))\n",
    "print('OOB accuracy: {:.3f}'.format(oob_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests (RF)\n",
    "- Ensemble method that uses a decision tree as the base estimator\n",
    "- Each estimator is trained on a different bootstrap sample having the same size as the training set\n",
    "- RF introduces further randomization in the training of individual trees\n",
    "    - d features are sampled at each node without replacement, where d is less than the total number of features\n",
    "- <b>Classification</b>\n",
    "    - Aggregates predictions by majority voting\n",
    "    - RandomForestClassifier in sklearn\n",
    "- <b>Regression</b>\n",
    "    - Aggregates predictions through averaging\n",
    "    - RandomForestRegressor in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of Random Forests: 4.16\n"
     ]
    }
   ],
   "source": [
    "# Import data and separate response and predictors\n",
    "import pandas as pd\n",
    "df = pd.read_csv('auto-mpg.csv')\n",
    "X = df.drop('mpg', axis=1)\n",
    "y = df['mpg']\n",
    "\n",
    "# Basic Imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set Seed\n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=SEED)\n",
    "\n",
    "# Instantiate a random forests regressor with 400 estimators\n",
    "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "# Fit rf to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of Random Forests: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "When a random forest model is trained, feature importance can be assessed\n",
    "- in sklearn:\n",
    "    - how much the tree nodes use a particular feature (weighted avg) to reduce impurity\n",
    "    - accessed using the attribute feature_importance_ on the model(ex. rf.feature_importance_)\n",
    "    \n",
    "To visualize the importance of features from rf...    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create a pd.Series of feature importances\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X_train.columns)\n",
    "\n",
    "# Sort importances_rf\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "# Make a horizontal bar plot\n",
    "sorted_importances_rf.plot(kind='barh')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 4 - Boosting\n",
    "Boosting: \n",
    "- Ensemble Method combining several weak learners to form a stronger learner\n",
    "- Weak Learner: Model doing just slightly better than random guessing (CART with maximum depth of 1)\n",
    "- Train an ensemble of predictors sequentially, with each predictor trying to correct its predecessor\n",
    "\n",
    "### AdaBoost (Adaptive Boosting)\n",
    "    - Each predictor pays more attention to the instances wrongly predicted by it's predecesso\n",
    "    - Achieved by changing the weights of training instances\n",
    "    - Each predictor is assigned a coefficient -- alpha\n",
    "    - alpha depends on the predictor's training error  \n",
    "- <b>Classification</b>\n",
    "    - Weighted majority voting\n",
    "    - AdaBoostClassifier in sklearn\n",
    "- <b>Regression</b>\n",
    "    - Aggregates predictions through averaging\n",
    "    - RandomForestRegressor in sklearn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost ROC AUC Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Import Metrics and splitting functions\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import models to use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Import Data\n",
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Set Seed \n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate the classification tree\n",
    "dtree = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "# Instantiate an Adaboost classifier\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dtree, n_estimators=100)\n",
    "\n",
    "# Fit adb_clf to training set\n",
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set probabilities of positive class\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,-1]\n",
    "\n",
    "# Evaluate the test set roc_auc_score\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print the evaluation result\n",
    "print('AdaBoost ROC AUC Score: {:.2f}'.format(adb_clf_roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (GB)\n",
    "- Each predictor corrects its predecessor's error\n",
    "- Does not tweak the weights of training instances\n",
    "- Fit each predictor is trained using its predecessor's residual errors as labels\n",
    "- Gradient Boosted Trees: base learner is a CART\n",
    "- <b>Regression</b>\n",
    "    - GradientBoostingRegressor in sklearn  \n",
    "- <b>Classification</b>\n",
    "    - GradientBoostingClassifier in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting test set RMSE: 3.07\n"
     ]
    }
   ],
   "source": [
    "# Import data and separate response and predictors\n",
    "import pandas as pd\n",
    "df = pd.read_csv('auto-mpg.csv')\n",
    "X = df.drop('mpg', axis=1)\n",
    "y = df['mpg']\n",
    "\n",
    "# Basic Imports\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set Seed\n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=SEED)\n",
    "\n",
    "# Instantiate a GradientBoostingRegressor\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
    "\n",
    "# Fit gbt to training set\n",
    "gbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the result\n",
    "print('Gradient Boosting test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting has some limitations\n",
    "- involves an exhaustive search procedure\n",
    "- Each CART is trained to find the best split points and features\n",
    "- May lead to CARTs using the same split points and maybe even the same features\n",
    "\n",
    "### Stochastic Gradient Boosting (SGB)\n",
    "- Each CART is trained on a random subset of training data\n",
    "- sampled without replacement\n",
    "- Features are sampled without replacement when choosing the best split points\n",
    "- results in further ensemble diversity\n",
    "- effect: adding further variance to ensemble of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGBT Test Set RMSE: 3.23\n"
     ]
    }
   ],
   "source": [
    "# Import data and separate response and predictors\n",
    "import pandas as pd\n",
    "df = pd.read_csv('auto-mpg.csv')\n",
    "X = df.drop('mpg', axis=1)\n",
    "y = df['mpg']\n",
    "\n",
    "# Basic Imports\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set Seed\n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=SEED)\n",
    "\n",
    "# Instantiate a stochastic GradientBoostingRegressor 'sgbt'\n",
    "# subsample choosing portion of data to sample for the tree\n",
    "# max_features choosing portion of available features to test\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, \n",
    "                                 n_estimators=300, random_state = SEED)\n",
    "\n",
    "# Fit to the training set\n",
    "sgbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "# Compute and print the RMSE for the sgbt model\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "print('SGBT Test Set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 5 - Model Tuning\n",
    "- Hyperparameters of a model should be tuned to obtain the best possible fit\n",
    "- Parameters: learned from data in training (split point in a node)\n",
    "- Hyperparameters: must be set or chosen (maximum depth of a decision tree)\n",
    "- Tuning:\n",
    "    - Problem: search for set of optimal hyperparameters for a learning algorithm\n",
    "    - Solution: find a set of optimal hyperparameters to make the optimal model\n",
    "    - Optimal Model: yields an optimal score\n",
    "    - Score: in sklearn this defaults to accuracy (classification) and R2 (regression) \n",
    "- Cross Validation is used to estimate generalization performance\n",
    "- Hyperparameters need to be tuned because the default values are often not optimal for all situations and problems\n",
    "\n",
    "### Approaches to Hyperparameter Tuning\n",
    "- GridSearch, RandomSearch, BaysianOptimization, GeneticAlgorithms, etc...\n",
    "\n",
    "## Grid Search cross validation\n",
    "- Manually set a grid of discrete hyperparameter values\n",
    "- Set a metric for scoring model performance\n",
    "- Search exhaustively through the grid\n",
    "- Evaluate the models CV score for each combination of hyperparameters\n",
    "- choose the best scoring combination as the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccp_alpha                            0\n",
      "class_weight                      None\n",
      "criterion                         gini\n",
      "max_depth                         None\n",
      "max_features                      None\n",
      "max_leaf_nodes                    None\n",
      "min_impurity_decrease                0\n",
      "min_impurity_split                None\n",
      "min_samples_leaf                     1\n",
      "min_samples_split                    2\n",
      "min_weight_fraction_leaf             0\n",
      "presort                     deprecated\n",
      "random_state                         1\n",
      "splitter                          best\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set SEED\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate the classifier\n",
    "dtree = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# Print out dt's hyperparameters\n",
    "print(pd.Series(dtree.get_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      " {'max_depth': 3, 'max_features': 0.4, 'min_samples_leaf': 0.04} \n",
      "\n",
      "Best CV Accuracy:  0.9428571428571428\n",
      "Test Set Accuracy of Best Model: 0.939\n"
     ]
    }
   ],
   "source": [
    "# Import Metrics and splitting functions\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Import models to use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import Data\n",
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Set Seed \n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate the classifier\n",
    "dtree = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "params_dtree = {\n",
    "                'max_depth':[3,4,5,6],\n",
    "                'min_samples_leaf':[0.04, 0.06, 0.08],\n",
    "                'max_features': [0.2, 0.4, 0.6, 0.8]\n",
    "               }\n",
    "\n",
    "# Instantiate a 10-folr CV GridSearch object\n",
    "grid_dtree = GridSearchCV(estimator=dtree,\n",
    "                          param_grid=params_dtree,\n",
    "                          scoring='accuracy',\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# Fit grid_dt to training data\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "# Extract best hyperparameter values from grid_dtree after training\n",
    "best_hyp = grid_dtree.best_params_\n",
    "print('Best Hyperparameters:\\n', best_hyp,'\\n')\n",
    "\n",
    "# Extract the best CV accuracy\n",
    "best_CV_score = grid_dtree.best_score_\n",
    "print('Best CV Accuracy: ', best_CV_score)\n",
    "\n",
    "# Extract best model\n",
    "best_model = grid_dtree.best_estimator_\n",
    "\n",
    "# Evaluate and print the test set accuracy\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "print('Test Set Accuracy of Best Model: {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameter Tuning\n",
    "- Same Hyperparameters as CARTs along with others such as n_estimators and bootstrap\n",
    "- Hyperparameter tuning can be very computationaally expensive, and sometimes only offer slight improvement\n",
    "- It is important to weight the impact of tuning to the pipeline of the whole project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "SEED = 1\n",
    "\n",
    "# Inspect Random Forest Hyperparameters\n",
    "rf = RandomForestRegressor(random_state = SEED)\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:   24.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      " max_depth              4\n",
      "max_features        log2\n",
      "min_samples_leaf     0.1\n",
      "n_estimators         300\n",
      "dtype: object \n",
      "\n",
      "Test set RMSE of rf: 4.03\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('auto-mpg.csv')\n",
    "X = df.drop('mpg', axis=1)\n",
    "y = df['mpg']\n",
    "\n",
    "# Basic Imports\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set Seed\n",
    "SEED = 1\n",
    "\n",
    "# Split data 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=SEED)\n",
    "\n",
    "# Instantiate the random forest regressor\n",
    "rf = RandomForestRegressor(random_state = SEED)\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "params_rf = {\n",
    "            'n_estimators':[300,400,500],\n",
    "            'max_depth':[4, 6, 8],\n",
    "            'min_samples_leaf':[0.1,0.2],\n",
    "            'max_features':['log2','sqrt']\n",
    "            }\n",
    "\n",
    "# Instantiate a GridSearchCV object\n",
    "grid_rf = GridSearchCV(estimator=rf, param_grid=params_rf,\n",
    "                       cv=3, scoring='neg_mean_squared_error',\n",
    "                       verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit grid_rf to the training set\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best hyperparameters from grid_rf\n",
    "best_hyp = grid_rf.best_params_\n",
    "print('Best Hyperparameters:\\n',pd.Series(best_hyp),'\\n')\n",
    "\n",
    "# Extract the best model\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate and print the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(0.5)\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0190b3ca094bdd020b32bd135a3f407c78fd7069e00b3ef5b2f0fe0fddf2602ff",
   "display_name": "Python 3.8.8 64-bit ('python_ML': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}