{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 1 - Basic Modeling in Scikit-learn\n",
    "\n",
    "### What is model validation\n",
    "- ensuring your model performs as expected on new data\n",
    "- testing model performance on holdout sets\n",
    "- selecting the best model, parameters, and accuracy metrics\n",
    "- achieving the best accuracy for the data given\n",
    "\n",
    "### Basic modeling steps\n",
    "- create a model by instantiating a model type and it's parameters (ex. rf = RandomForestRegressor(n_estimators=500))\n",
    "- fit the model to a training set of data (ex. rf.fit(X_train, y_train)\n",
    "- generate predictions for the testing set of data (ex. y_pred = rf.predict(X_test))\n",
    "- assess the accuracy metrics (ex. MSE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('candy-data.csv')\n",
    "ttt = pd.read_csv('tic-tac-toe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing model accuracy on seen vs unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model error on seen data: 3.59.\n",
      "Model error on unseen data: 9.49.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports to use\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error as  mae\n",
    "\n",
    "# Split the data into predictors and response\n",
    "X = candy.drop(['competitorname', 'winpercent'], axis=1)\n",
    "y = candy['winpercent']\n",
    "\n",
    "# Create a training and testing set of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1111)\n",
    "\n",
    "# Instantiate the model\n",
    "rf = RandomForestRegressor(random_state=1111)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on both the training and testing set\n",
    "train_pred = rf.predict(X_train)\n",
    "test_pred = rf.predict(X_test)\n",
    "rf.score\n",
    "\n",
    "# Calculate errors for train and test predictions\n",
    "train_error = mae(y_train, train_pred)\n",
    "test_error = mae(y_test, test_pred)\n",
    "\n",
    "# Print the accuracy for seen and unseen data\n",
    "print(\"Model error on seen data: {0:.2f}.\".format(train_error))\n",
    "print(\"Model error on unseen data: {0:.2f}.\\n\".format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models - RandomForestRegressor\n",
    "We will focus on 3 hyperparameters\n",
    "- n_estimators: number of trees in the forest\n",
    "- max_depth: maximum depth of the trees (levels)\n",
    "- random_state: random seed to ensure reproducibility\n",
    "\n",
    "You can set these hyperparameters in two ways\n",
    "- when instantiating the model: rf = RandomForestRegressor(n_estimators=500)\n",
    "- after the model is created: rf.n_estimators = 50 or rf.max_depth = 10\n",
    "    - this could be helpful when testing out different sets of parameters\n",
    "\n",
    "Feature Importance can be assessed to see how each feature contributed to the model\n",
    "- for i, item in enumerate(rf.feature_importances_): print('{}: {:.2f}'.format(X.columns[i], item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Scores:\n",
      "chocolate: 0.43\n",
      "fruity: 0.04\n",
      "caramel: 0.02\n",
      "peanutyalmondy: 0.08\n",
      "nougat: 0.00\n",
      "crispedricewafer: 0.02\n",
      "hard: 0.01\n",
      "bar: 0.03\n",
      "pluribus: 0.01\n",
      "sugarpercent: 0.20\n",
      "pricepercent: 0.16\n"
     ]
    }
   ],
   "source": [
    "# Print the Feature Importances from the model created above\n",
    "print('Feature Importance Scores:')\n",
    "for i, item in enumerate(rf.feature_importances_):\n",
    "    print('{}: {:.2f}'.format(X.columns[i], item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models - RandomForestClassifier\n",
    "We'll use the tic-tac-toe dataset since it is a complete data set of all possible game combinations.\n",
    "- x = player one\n",
    "- o = player two\n",
    "- b = blank space at end of game\n",
    "- positive = player one wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Tic-Tac-Toe data for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn Class into binary (positive=1, negative=0)\n",
    "ttt.replace('positive', 1, inplace=True)\n",
    "ttt.replace('negative', 0, inplace=True)\n",
    "\n",
    "# Get Dummies for other features\n",
    "ttt_prep = pd.get_dummies(ttt, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Score:  1.0\n",
      "Test Set Score:  0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports to use\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error as  mae\n",
    "\n",
    "# Split the data into predictors and response\n",
    "X = ttt_prep.drop(['Class'], axis=1)\n",
    "y = ttt_prep.Class\n",
    "\n",
    "# Create a training and testing set of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1111)\n",
    "\n",
    "# Instantiate the model\n",
    "rf = RandomForestClassifier(random_state=1111)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Score the accuracy\n",
    "print('Training Set Score: ',rf.score(X_train, y_train))\n",
    "print('Test Set Score: ',rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 2 - Validation Basics\n",
    "## Holdout Samples\n",
    "- Training set: Used to build and train models\n",
    "- Validation set: used to assess and compare models, and to tune hyperparameters\n",
    "- Testing set: Used to assess the final models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into predictors and response\n",
    "X = ttt_prep.drop(['Class'], axis=1)\n",
    "y = ttt_prep.Class\n",
    "\n",
    "# Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split X into temporary and test sets, then split the temp set into train and validation sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metrics\n",
    "### Regression\n",
    "The metric you pick is application specific. These are measured in different units and should not be compared\n",
    "- <b>Mean Absolute Error (MAE)</b>:\n",
    "    - simplest and most intuitive\n",
    "    - treats all points equally\n",
    "    - not sensitive to outliers\n",
    "- <b>Mean Squared Error (MSE)</b>:\n",
    "    - most widely used regression metric\n",
    "    - larger errors have a higher impact (due to squaring)\n",
    "    - more sensitive to outliers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 9.91\n",
      "Mean Squared Error (MSE): 144.10\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports to use\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error as  MAE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Split the data into predictors and response\n",
    "X = candy.drop(['competitorname', 'winpercent'], axis=1)\n",
    "y = candy['winpercent']\n",
    "\n",
    "# Create a training and testing set of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1111)\n",
    "\n",
    "# Instantiate the model\n",
    "rf = RandomForestRegressor(random_state=1111)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on both the training and testing set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate and print error with MAE and MSE\n",
    "mae_rf = MAE(y_test, y_pred)\n",
    "mse_rf = MSE(y_test, y_pred)\n",
    "\n",
    "print('Mean Absolute Error (MAE): {:.2f}'.format(mae_rf))\n",
    "print('Mean Squared Error (MSE): {:.2f}'.format(mse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "Precision, Recall, Accuracy, Speciificity, F1-Score (and it's variations), and many more\n",
    "##### Confusion Matrix can help with these scores (from Tic-Tac-Toe classification above)\n",
    "[[204  60]\n",
    " \n",
    " [ 19 484]]\n",
    "- <b>Precision</b>:\n",
    "    - number of true positives out of all predicted positive values\n",
    "    - Used when we don't want to over predict positive values\n",
    "    - 484 / (484 + 60) = 0.89\n",
    "- <b>Recall (Sensitivity)</b>:\n",
    "    - portion of true positives out of all possible possitives\n",
    "    - Used when we can't afford to miss any positive values (medical diagnosis)\n",
    "    - 484 / (484 + 19) = 0.962\n",
    "- <b>Accuracy</b>:\n",
    "    - Proportion of correct predictions compared to entire sample\n",
    "    - (204 + 484) / (204 + 60 + 19 + 484) = 0.897\n",
    "    \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff\n",
    "- Variance: \n",
    "    - model pays too close of attention to the training data\n",
    "    - fails to generalize\n",
    "    - low training error but high testing error\n",
    "    - Over Fit with high complexity  \n",
    "- Bias:\n",
    "    - Model fails to find relationships between data and response\n",
    "    - high errors on both training and testing\n",
    "    - Under Fit with low complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training scores were: [0.94, 0.94, 0.99, 0.98, 1.0, 1.0, 1.0, 1.0]\n",
      "The testing scores were: [0.83, 0.86, 0.93, 0.93, 0.93, 0.96, 0.97, 0.99]\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports to use\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into predictors and response\n",
    "X = ttt_prep.drop(['Class'], axis=1)\n",
    "y = ttt_prep.Class\n",
    "\n",
    "# Create a training and testing set of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "# Use a for loop to test different accuracy scores for a range of n_estimators\n",
    "test_scores, train_scores = [], []\n",
    "for i in [1, 2, 3, 4, 5, 10, 20, 50]:\n",
    "    rfc = RandomForestClassifier(n_estimators=i, random_state=1111)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # Create predictions for the X_train and X_test datasets.\n",
    "    train_predictions = rfc.predict(X_train)\n",
    "    test_predictions = rfc.predict(X_test)\n",
    "    # Append the accuracy score for the test and train predictions.\n",
    "    train_scores.append(round(accuracy_score(y_train, train_predictions), 2))\n",
    "    test_scores.append(round(accuracy_score(y_test, test_predictions), 2))\n",
    "    \n",
    "# Print the train and test scores.\n",
    "print(\"The training scores were: {}\".format(train_scores))\n",
    "print(\"The testing scores were: {}\".format(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with only one tree, both the train and test scores are low. As you add more trees, both errors improve. Even at 50 trees, this still might not be enough. Every time you use more trees, you achieve higher accuracy. At some point though, more trees increase training time, but do not decrease testing error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 3 Cross-Validation\n",
    "- Holdout samples run the risk of holding out data that could have key effects on the model building and predictions\n",
    "- something as simple as changing a random seed state can drastically affect the performance of a model\n",
    "\n",
    "## Cross validation: the gold standard\n",
    "cross_val_score\n",
    "- estimator - model to use\n",
    "- X - complete training set\n",
    "- y - response variable data set\n",
    "- cv (number of folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152.03588775 103.95118251  89.34700797 206.07148616 140.8862936 ]\n",
      "Cross Val Mean: 138.45837159620368\n",
      "Cross Val Std: 40.90097589634289\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "# Split the data into predictors and response\n",
    "X = candy.drop(['competitorname', 'winpercent'], axis=1)\n",
    "y = candy['winpercent']\n",
    "\n",
    "# Instantiate the regressor\n",
    "rf = RandomForestRegressor(n_estimators=20, max_depth=5, random_state=1111)\n",
    "\n",
    "# Make scorer for Cross validation (optional)\n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# Run cross validation\n",
    "cv_results = cross_val_score(rf, X, y, cv=5, scoring=mse)\n",
    "\n",
    "# Evaluate results\n",
    "print(cv_results)\n",
    "print('Cross Val Mean: {}'.format(cv_results.mean()))\n",
    "print('Cross Val Std: {}'.format(cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out Cross Validation (LOOCV)\n",
    "k-Fold CV where k=n. This means every point will be used in a validation set all by itself. Because of this it is very computationally expensive\n",
    "\n",
    "- Use when:\n",
    "    - data is limited\n",
    "    - want to use as much data for training as possible\n",
    "- Don't use when:\n",
    "    - computation resources are limited\n",
    "    - large datasets\n",
    "    - lots of parameters to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Mean: 134.12865873926833\n",
      "Cross Val Std: 180.11962158885254\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "# Split the data into predictors and response\n",
    "X = candy.drop(['competitorname', 'winpercent'], axis=1)\n",
    "y = candy['winpercent']\n",
    "\n",
    "# Instantiate the regressor\n",
    "rf = RandomForestRegressor(n_estimators=20, max_depth=5, random_state=1111)\n",
    "\n",
    "# Make scorer for Cross validation (optional)\n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# Run cross validation, using the number of observations as the number of folds\n",
    "n = X.shape[0]\n",
    "cv_results = cross_val_score(rf, X, y, cv=n, scoring=mse)\n",
    "\n",
    "# Evaluate results\n",
    "print('Cross Val Mean: {}'.format(cv_results.mean()))\n",
    "print('Cross Val Std: {}'.format(cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 4 - Hyperparameter Tuning\n",
    "Parameters\n",
    "- learned or calculated by the algorithm based on training data\n",
    "\n",
    "Hyperparametrs\n",
    "- Set manually by the modeler to tune the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
