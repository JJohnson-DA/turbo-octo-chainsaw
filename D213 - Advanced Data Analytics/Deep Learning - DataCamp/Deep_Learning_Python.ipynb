{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 1 Basics of Deep Learning and Neural Networks\n",
    "- Neural Networks are a powerful modeling approach that accounts for interaction in the model really well\n",
    "- Deep Learning uses especially powerful neural networks\n",
    "    - Text, images, videos, audio, source code, and really anything else\n",
    "- Neural Network Structure\n",
    "    - input layer\n",
    "    - hidden layer(s): consists of nodes that represent aggregations of information from our input data. More Nodes generally means the model can account for more interactions\n",
    "    - output layer\n",
    "\n",
    "### Forward Propogation\n",
    "- Input data are multiplied by weights and added together at hidden layer nodes, this continues for each node going forward in the each hidden layer until the output is reached.\n",
    "\n",
    "Bank transaction example\n",
    "- Make predictions based on:\n",
    "    - Number of Children\n",
    "    - Number of existing accounts\n",
    "    \n",
    "#### Writing Code to forward propogate a small neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39\n"
     ]
    }
   ],
   "source": [
    "# Input data and weights from DataCamp\n",
    "input_data = np.array([3,5])\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "# Calculate node 0 value: node_0_value\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "\n",
    "# Calculate node 1 value: node_1_value\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_value, node_1_value])\n",
    "\n",
    "# Calculate output: output\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "- Applied in the hidden layers and allows the model to capture non-linearity\n",
    "- if the relationships in the data are not straight line functions, we need activation functions that can capture the non-linearity\n",
    "- Applied to the input coming into the node, the result is stored and used at that nodes output\n",
    "- Standard today is the Rectified Linear Activation (ReLU) Function\n",
    "    - relu = 0 if x < 0, and x if x > 0\n",
    "        - relu(3) = 3\n",
    "        - relu(-3) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(input, 0)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "# Input data and weights from DataCamp\n",
    "input_data = np.array([3,5])\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! You predicted 52 transactions. Without this activation function, you would have predicted a negative number! The real power of activation functions will come soon when you start tuning model weights.\n",
    "\n",
    "##### Applying the network to many observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 0, 0, 64]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([[3, 5], [ 1, -1],[0, 0], [8, 4]])\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (node_0_output * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Networks\n",
    "- Neural Networks have become much better as we've been able to add more hidden layers\n",
    "- subsequent hidden layers take the output of previous nodes as the input, until eventually reaching the output node, or result\n",
    "\n",
    "Representation Learning\n",
    "- Deep Networks internally build representations of patterns in the data\n",
    "- Partially replace the need for feature engineering\n",
    "- Subsequent layers build increasingly sophisticated representations of the raw data, until we get to the prediction stage\n",
    "- modeler does not need to specify what interactions to look for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3,5])\n",
    "weights = {'node_0_0': np.array([2, 4]), 'node_0_1': np.array([ 4, -5]),\n",
    "           'node_1_0': np.array([-1, 2]), 'node_1_1': np.array([1, 2]),\n",
    "           'output': np.array([2, 7])}\n",
    "\n",
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 2 Optimizing Neural Network with Backward Propogation\n",
    "### The Need for Optimization\n",
    "- The perfect weights for one data point are unlikely to be perfect for another\n",
    "- When developing a model based on multiple points, you want to find the weights that minimize the loss function.\n",
    "- Goal: find the weights that give the lowest value for the loss function\n",
    "- <b>Gradient Descent</b> Algorithm: aims to find the lowest value. can think of as finding the bottom of a valley. If the ground is very steep you can take a larger step down the hill before measuring. As the ground becomes more flat you will take smaller steps. This continues in the shrinking of step sizes until you find that any more steps, no matter how small, will cause you to move uphill and furthest from the lowest point\n",
    "    - start at a random point\n",
    "    - find the slope\n",
    "    - take a step\n",
    "    - repeat until at lowest point\n",
    "    \n",
    "#### Coding how Weight changes accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (node_0_output * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# The data point you will make a prediction for\n",
    "input_data = np.array([0, 3])\n",
    "\n",
    "# Sample weights\n",
    "weights_0 = {'node_0': np.array([2, 1]),\n",
    "             'node_1': np.array([1, 2]),\n",
    "             'output': np.array([1, 1])\n",
    "            }\n",
    "\n",
    "# The actual target value, used to calculate the error\n",
    "target_actual = 3\n",
    "\n",
    "# Make prediction using original weights\n",
    "model_output_0 = predict_with_network(input_data, weights_0)\n",
    "\n",
    "# Calculate error: error_0\n",
    "error_0 = model_output_0 - target_actual\n",
    "\n",
    "# Create weights that cause the network to make perfect prediction (3): weights_1\n",
    "weights_1 = {'node_0': np.array([2, 1]),\n",
    "             'node_1': np.array([1, 2]),\n",
    "             'output': np.array([1, 0])\n",
    "            }\n",
    "\n",
    "# Make prediction using new weights: model_output_1\n",
    "model_output_1 = predict_with_network(input_data, weights_1)\n",
    "\n",
    "# Calculate error: error_1\n",
    "error_1 = model_output_1 - target_actual\n",
    "\n",
    "# Print error_0 and error_1\n",
    "print(error_0)\n",
    "print(error_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling to multiple Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error with weights_0: 235.000000\n",
      "Mean squared error with weights_1: 354.390625\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array(([0, 3], [1, 2], [-1, -2], [4, 0]))\n",
    "weights_0 = {'node_0': np.array([2, 1]), \n",
    "             'node_1': np.array([1, 2]), \n",
    "             'output': np.array([1, 1])\n",
    "            }\n",
    "weights_1 = {'node_0': np.array([2, 1]),\n",
    "             'node_1': np.array([1. , 1.5]),\n",
    "             'output': np.array([1. , 1.5])\n",
    "            }\n",
    "target_actuals = [1, 3, 5, 7]\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create model_output_0 \n",
    "model_output_0 = []\n",
    "# Create model_output_1\n",
    "model_output_1 = []\n",
    "\n",
    "# Loop over input_data\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network(row, weights_0))\n",
    "    \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network(row, weights_1))\n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_0 = mean_squared_error(target_actuals, model_output_0)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_1 = mean_squared_error(target_actuals, model_output_1)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation\n",
    "- Takes the error from the output layer and sends it backwards through the hidden layers to the input layer\n",
    "- allows gradient descent to update all weights in neural network (by getting gradients for all weights)\n",
    "- comes from chain rule of calculus\n",
    "- Important to understand the process, but you will generally use a library to implent this\n",
    "\n",
    "Process\n",
    "- Backpropogation is trying to estimate the slope of the loss function with respect to each weight\n",
    "- Do forward propogation to calculate predictions and errors before backpropogation\n",
    "- Go back one layer at a time\n",
    "- Gradient for weight is the product of:\n",
    "    - Node value feeding into that weight\n",
    "    - Slope of loss function with respect to the node it feeds into\n",
    "    - Slope of activation function at the node it feeds into\n",
    "- Need to also keep track of the slopes of the loss function with respect to node values\n",
    "- Slope of node values are the sum of the slopes for all weights that come out of them\n",
    "\n",
    "Recap of Backpropogation\n",
    "- Start at some random set of weights\n",
    "- use forward propogation to make a prediction\n",
    "- use backward propogation to calculate the slope of the loss funciton with respect to each weight\n",
    "- multiple that slope by the learning rate, and subract from the current weights\n",
    "- repeat cycle until we get to a \"flat\" part of the curve\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "- calculate the slopes on only a subset of the data, or \"batch\"\n",
    "- use a different batch of data to calculate the next update\n",
    "- start over from the beginning once all data is used\n",
    "- each time through the training data is called an epoch\n",
    "- Slopes are calculated on one batch at a time: stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 3 Building Models with Keras\n",
    "## Model Building Steps\n",
    "- Specify Architecture: number of layers, number of nodes, activation function\n",
    "- Compile the model: specify loss function and details about optimization\n",
    "- Fit the model: cycle of forward and backward propogation\n",
    "- Predict\n",
    "\n",
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('wages.txt')\n",
    "target = df['wage_per_hour']\n",
    "predictors = df.drop('wage_per_hour', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('wages.txt')\n",
    "target = df['wage_per_hour']\n",
    "predictors = df.drop('wage_per_hour', axis=1)\n",
    "\n",
    "# Find the number of nodes in the input layer, equal to number of input features\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Dense means all nodes will connect to each other node in the next layer\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,))) # connects all input into 100 nodes\n",
    "model.add(Dense(32, activation='relu')) # Connects all 100 input nodes to all 100 in this layer\n",
    "model.add(Dense(1, activation='relu')) # Converges into 1 final output node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "#### Why you need to compile the model\n",
    "- Specify the Optimizer\n",
    "    - Many options and mathematically complex\n",
    "    - best to choose versatile option and use that for most problems\n",
    "    - \"Adam\" is usually a good choice. It adjusts the learning rate as it does gradient descent to ensure reasonable values throughout the weight optimization process\n",
    "- Loss Function\n",
    "    - MSE is common for regression problems\n",
    "    - Classificatino has a different default metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Fitting a model\n",
    "- applying backpropogation and gradient descent with your data to update the weights\n",
    "- Scaling data before fitting can ease optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 931us/step - loss: 107.7946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29627c44040>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Process - Specify, Compile, Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 739us/step - loss: 50.4915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x296299d40d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='relu')) \n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Fit the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models\n",
    "- 'categorical_crossentropy' as the loss function. Lower score is better\n",
    "- Add Metrics=['accuracy'] to compile step for easy to understand diagnostics\n",
    "- Output layer has seperate node for each possible outcome, and use \"softmax\" activation function in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/28 [==============================] - 0s 770us/step - loss: 2.9524 - accuracy: 0.5915\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 0s 583us/step - loss: 0.9866 - accuracy: 0.6442\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 0s 570us/step - loss: 0.7217 - accuracy: 0.6588\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 0s 570us/step - loss: 0.6679 - accuracy: 0.6779\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 0s 581us/step - loss: 0.6317 - accuracy: 0.7015\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 0s 641us/step - loss: 0.6181 - accuracy: 0.6970\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 0s 641us/step - loss: 0.5951 - accuracy: 0.6992\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 0s 606us/step - loss: 0.6098 - accuracy: 0.6936\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 0s 677us/step - loss: 0.6099 - accuracy: 0.7003\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 0s 570us/step - loss: 0.5965 - accuracy: 0.7026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2962ac555b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('titanic.txt')\n",
    "target = to_categorical(df.survived)\n",
    "predictors = df.drop('survived', axis=1)\n",
    "predictors.replace(False, 0, inplace=True)\n",
    "predictors.replace(True, 1, inplace=True)\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors, target, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Models\n",
    "- Save model after training\n",
    "- Reload the model\n",
    "- Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26049826 0.43580088 0.5783743  0.5376731 ]\n"
     ]
    }
   ],
   "source": [
    "pred_data = np.array([[2, 34.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "       [2, 31.0, 1, 1, 26.25, 0, False, 0, 0, 1],\n",
    "       [1, 11.0, 1, 2, 120.0, 1, False, 0, 0, 1],\n",
    "       [3, 0.42, 0, 1, 8.5167, 1, False, 1, 0, 0]])\n",
    "\n",
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(pred_data)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 4 Fine Tuning Keras Models\n",
    "Why optimization is hard\n",
    "- simultaneously optimizing 1000s of parameters with complex relationships\n",
    "- Updates may not improve model meaningfully\n",
    "- updates too small (if learning rate is low) or too large (if learning rate is high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 7.8156\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 0s 712us/step - loss: 7.7668\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 0s 677us/step - loss: 7.7181\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 0s 712us/step - loss: 7.6695\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 0s 720us/step - loss: 7.6210\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 0s 897us/step - loss: 1.2399\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 0s 606us/step - loss: 0.6897\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 0s 677us/step - loss: 0.6796\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 0s 712us/step - loss: 0.6297\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 0s 683us/step - loss: 0.6036\n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 0s 677us/step - loss: 4089.2920\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 0s 748us/step - loss: 0.6716\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 0s 677us/step - loss: 0.6708\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 0s 748us/step - loss: 0.6687\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 0s 677us/step - loss: 0.6712\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('titanic.txt')\n",
    "target = to_categorical(df.survived)\n",
    "predictors = df.drop('survived', axis=1)\n",
    "predictors.replace(False, 0, inplace=True)\n",
    "predictors.replace(True, 1, inplace=True)\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "def get_new_model(n_cols):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [0.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model(n_cols)\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors, target, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "### validation in Deep Learning\n",
    "- commonly use a validation split rather than cross validation\n",
    "- Because Deep Learning widely used on large datasets and cross-validation would be computationally expensive\n",
    "- Single validation score is based on large amount of data, and is reliable\n",
    "- \"validation_split\" can be used in fitting of model and takes a decimal for what fraction to use in validation\n",
    "\n",
    "Early Stopping can be used to make sure we stop optimizing the model once the best validation score is reached. Patience is how many epochs the model can go without improving before stopping.\n",
    "- from keras.callbacks import EarlyStopping\n",
    "- early_stopping_monitor = EarlyStopping(patience=2)\n",
    "- model.fit(predictors, target, validation_split=0.3,\n",
    "            nb_epoch=20, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.1404 - accuracy: 0.5490 - val_loss: 0.8972 - val_accuracy: 0.6082\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7179 - accuracy: 0.6116 - val_loss: 0.5897 - val_accuracy: 0.7164\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.6549 - val_loss: 0.5649 - val_accuracy: 0.7090\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7033 - accuracy: 0.6597 - val_loss: 0.5801 - val_accuracy: 0.7351\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6156 - accuracy: 0.6709 - val_loss: 0.5587 - val_accuracy: 0.6978\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6116 - accuracy: 0.7047 - val_loss: 0.5233 - val_accuracy: 0.7425\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.7047 - val_loss: 0.5045 - val_accuracy: 0.7201\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.7223 - val_loss: 0.5823 - val_accuracy: 0.7425\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.6982 - val_loss: 0.6526 - val_accuracy: 0.6567\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "# Load Data\n",
    "df = pd.read_csv('titanic.txt')\n",
    "target = to_categorical(df.survived)\n",
    "predictors = df.drop('survived', axis=1)\n",
    "predictors.replace(False, 0, inplace=True)\n",
    "predictors.replace(True, 1, inplace=True)\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model\n",
    "model = model.fit(predictors, target, \n",
    "                  validation_split=0.3, epochs=30,\n",
    "                  callbacks=[early_stopping_monitor],\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Thinking about Model Capacity\n",
    "### Workflow for optimizing model capacity\n",
    "- start with a small network and get the validation score\n",
    "- gradually increase capacity as long as score keeps improving\n",
    "- Keep increasing capacity until validation score is no longer improving\n",
    "\n",
    "## Stepping up to images\n",
    "Recognizing handwritten digits\n",
    "- MNIST Dataset\n",
    "- 28 x 28 grid flattened to 784 values for each image\n",
    "- value in each part of array denotes darkness of that pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c6e990981432>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Import and format data\n",
    "df = pd.read_csv('mnist.txt', header=None)\n",
    "y = to_categorical(df[0])\n",
    "X = df.drop([0], axis=1)\n",
    "n_cols=X.shape[1]\n",
    "\n",
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, validation_split=0.3, epochs=30, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
