{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd5caba-a799-4940-84bc-344a9fdc4e68",
   "metadata": {},
   "source": [
    "# Ch. 1 Regular Expressions and word Tokenization\n",
    "## What is Natural Language Processing?\n",
    "- field of study focused on making sense of language using statistics and computers\n",
    "- basics of NLP\n",
    "    - Topic Identification\n",
    "    - Text Classification\n",
    "- NLP Applications\n",
    "    - Chatbots\n",
    "    - Translation\n",
    "    - Sentiment Analysis\n",
    "    \n",
    "## Regular Expressions\n",
    "- strings with a special syntax\n",
    "- allow us to match patterns in other strings\n",
    "- Applications\n",
    "    - find links in a webpage\n",
    "    - parse email addresses\n",
    "    - remove/replace unwanted characters\n",
    "- Import regex library in python\n",
    "    - import re\n",
    "- Common Regex Patterns\n",
    "    - \\w+ \n",
    "        - used to match words\n",
    "    - \\d \n",
    "        - used to match digits\n",
    "    - \\s\n",
    "        - matches spaces\n",
    "    - .*\n",
    "        - wildcard, matches any letter or symbol\n",
    "    - + or *\n",
    "        - greedy match, grabbing repeats of single letters or whole patterns\n",
    "        - adding + after \\w matches the full word rather than first character\n",
    "    - Using these character classes as capital letters negates them\n",
    "        - \\S will match anything that is not a space\n",
    "    - [a-z]\n",
    "        - lowercase group of characters\n",
    "        - ex ['abcdef']\n",
    "\n",
    "Python's re module\n",
    "- re: module\n",
    "- split: split a string on regex\n",
    "- findall: find all patterns in a string\n",
    "- search: search for a pattern. does not have to match at the beginning of the string\n",
    "- match: match an entire string or substring based on a pattern\n",
    "- When using re module, pass the pattern first, and the string second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f9f807-60bf-4268-9f29-35721c07f23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'Spaces']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.split('\\s+', 'Split on Spaces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034888d6-6b61-4dc5-836f-75353a9dfb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'RegEx']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!\"\n",
    "re.findall('\\w+', my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de116da-54d8-400c-a669-eb6892bdfe1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a22aee-0b86-44a5-8834-5b966d62597d",
   "metadata": {},
   "source": [
    "## Intro to Tokenization\n",
    "- Turning a string or document into tokens (smaller chunks)\n",
    "- One step in preparing a text for NLP\n",
    "- many different theories and rules for tokenization\n",
    "- you can create your own rules using regular expressions\n",
    "- Examples\n",
    "    - breaking out words or sentences\n",
    "    - separating punctuation\n",
    "    - separating out hashtags in a tweet\n",
    "- nltk is a common library for tokenization\n",
    "    - Natural Language Tookkit (nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3216f1dc-3997-4957-a2d4-4a2e5fd9bdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('Hi there!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a936620-cd8f-4753-a0aa-a5816050db67",
   "metadata": {},
   "source": [
    "### Why tokenize?\n",
    "- easier to map part of speech\n",
    "- matching common words\n",
    "- removing unwanted tokens such as common words or repeated words\n",
    "- Other nltk tokenizers\n",
    "    - sent_tokenize: tokenize a document into sentences\n",
    "    - regexp_tokenize: tokenize a string or document based on a regular expression pattern. Allows more granular control over the process\n",
    "    - TweetTokenizer: special class just for tweet tokenization, allowing you to separate hashtags, mentions, and lots of exclamation points\n",
    "\n",
    "### More Regex Practice\n",
    "- Differnce between re.search() and re.match()\n",
    "    - match starts at the beginning of the string to look for a match until it cannot match any longer\n",
    "        - use to be specific about composition of entire string, or at least the beginning\n",
    "        - re.match('abc', 'abcde') = 'abc'\n",
    "    - search looks for the pattern at any point in the string, not just the beginning\n",
    "        - use to find pattern that may not be at the beginning of a string\n",
    "        - re.search('abd', '123abcde = 'abc'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485b12e2-21cc-4ef2-954c-e770b1cdcad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eisrequiem', 'valleys', 'cart', 'See', 'four', 'Make', 'Leaving', 'behaviour', 'known', 'cross', 'KNIGHT', 'well', 'Since', 'ptoo', 'uh', 'Said', 'vital', 'SCENE', 'bells', 'orangutans', 'carrying', 'creep', 'Charge', 'Aaaaaaaaah', 'compared', 'lady', 'pig', 'Follow', 'Skip', 'everything', 'why', 'accomplished', 'had', 'together', 'tops', 'down', 'would', 'aside', \"'Here\", 'Listen', 'without', 'logically', 'THE', 'sniff', 'pray', 'lobbed', 'mercy', 'bugger-folk', 'dear', 'soon', 'Princess', 'Say', 'cut', 'PIGLET', 'Dingo', 'bottom', 'sex', 'biggest', 'things', 'honored', 'carve', 'castle', 'rocks', 'soiled', 'how', 'spirit', 'curtains', 'drink', 'language', 'with', 'out', 'worry', 'swallow', 'meeting', 'Hooray', 'nibble', 'rhymes', 'follow', 'Chapter', 'ARTHUR', 'Oooh', 'brought', 'set', 'minute', 'lovely', 'visually', 'case', 'islands', 'Just', 'between', 'explain', 'enough', 'God', 'risk', 'VILLAGER', 'praised', 'knew', 'Quickly', 'g', 'economic', 'dare', 'about', 'quack', 'footwork', 'impeccable', 'straight', 'her', 'coming', 'wherein', 'w', 'nasty', 'self-perpetuating', 'spooky', 'took', 'frighten', 'cadeau', 'WIFE', 'quarrel', 'This', 'B', 'Pull', 'oh', 'Ho', 'rode', 'bravest', 'closest', 'GUARD', 'daring', 'seldom', 'conclusions', 'bravely', 'ai', 'gravy', 'breadth', 'society', 'command', 'Bad', 'Silence', 'mumble', 'best', 'Ahh', 'occasion', 'GUARDS', 'second', 'laurels', 'sell', 'finds', 'Keep', 'classes', 'PRINCE', 'temperate', 'cartoon', 'wind', 'GUEST', 'daughter', 'English', 'animator', 'watery', 'One', 'ratified', 'wedlock', 'bed-wetting', 'fortune', 'work', 'Anthrax', 'keeper', 'fourth', 'north', 'France', 'European', 'lonely', 'minutes', 'wishes', 'carved', 'Practice', 'grenade', 'by', 'going', 'auntie', 'worked', 'perilous', 'relax', 'towards', 'Thou', 'ho', 'Could', 'task', 'pimples', 'heeh', 'blow', 'while', ';', 'note', 'chance', 'testicles', 'line', 'silly', 'Fetchez', 'spanking', 'LAUNCELOT', 'doors', 'police', 'excepting', 'bit', 'vote', 'gra', 'split', 'True', 'those', 'Riiight', 'Ay', 'fart', 'art', 'Mmm', 'But', 'even', 'whom', 'door-opening', 'rather', 'fellows', 'anything', 'terribly', 'Ulk', 'woman', 'unladen', 'Never', 'repressing', 'further', 'witness', 'pointy', 'shut', 'friend', 'CAMERAMAN', 'git', 'scared', 'aptly', 'nobody', 'havin', 'Spring', 'alarm', 'answer', 'eat', 'are', 'at', 'scene', 'Clear', 'Hang', 'pond', 'beat', 'DIRECTOR', 'signifying', 'window-dresser', 'understand', \"'uuggggggh\", 'comin', 'etc', 'lies', 'Aaaaugh', 'woosh', 'GALAHAD', 'Uhh', 'bite', 'no', 'progress', 'roar', 'lying', 'trumpets', 'leads', 'requiem', 'c', 'Would', 'He', 'Bloody', 'clap', 'wait', 'hang', 'summon', 'herring', 'guarded', 'haw', 'Come', 'mandate', \"'ni\", 'wise', 'Thy', 'pansy', 'warned', 'nineteen-and-a-half', 'rejoicing', 'creak', 'whinny', 'samite', 'sacrifice', 'knight', 'RANDOM', 'CRAPPER', 'valor', 'bint', 'Joseph', 'far', '18', 'arm', 'Forgive', 'live', 'crash', 'less', 'horrendous', 'strength', 'glory', 'proved', 'simple', 'brunettes', 'horn', 'nice', 'Yapping', 'music', 'disheartened', 'basic', 'penalty', 'shalt', 'Hoo', 'push', 'doing', 'ways', \"'First\", 'than', 'silence', 'go', 'daft', 'watch', 'raped', 'BRIDE', 'LEFT', 'Ask', 'asking', 'sorry', 'Hoa', 'ponds', 'Quick', 'your', 'thanks', 'dynamite', 'Olfin', 'obviously', 'Is', 'leap', 'Shut', 'stupid', 'pounds', 'regulations', 'examine', 'Am', 'Enchanter', 'mer', 'unarmed', 'gallantly', 'Packing', 'Gallahad', 'able', 'icy', 'Now', 'goes', 'zone', 'boys', 'waste', 'Three', 'Hya', 'PERSON', 'fruit', 'mistake', 'naughty', 'saved', 'Alright', 'forward', 'ARMY', '10', 'Chop', 'us', 'after', 'dance', 'gentle', 'awhile', 'Bring', 'gon', 'suffered', 'speak', 'Wayy', 'anchovies', 'Thank', 'Anarcho-syndicalism', 'act', 'outdated', 'looking', 'Thsss', 'alight', 'Tell', 'auuuuuuuugh', 'south', 'writing', 'lose', 'Sorry', 'Y', 'Have', 'father', 'Or', 'Nu', 'Hee', 'verses', 'big', 'Action', 'mud', 'Message', 'ever', 'ha', 'Who', 'Quoi', 'heart', 'flesh', 'soft', 'Splendid', 'streak', 'where', 'All', 'Aaaaaah', 'Pie', 'Try', 'wants', 'Umm', 'water', 'ZOOT', 'attack', 'retreat', 'In', 'l', 'birds', 'die', 'rewr', 'happy', 'upon', 'required', 'Court', 'Therefore', 'opera', 'bladders', 'radio', 'weighs', 'CRASH', 'Ooh', 'burned', 'crossed', 'bird', 'va.', 'triumphs', 'sworn', 'favor', 'laughing', 'STUNNER', 'nearer', 'Summer', 'thonk', 'vain', 'CUSTOMER', 'Oui', 'singing', 'conclusion', 'strewn', 'high', 'swords', 'Churches', 'biters', 'pass', 'scholar', 'SHRUBBER', 'bones', 'exciting', 'Thee', 'scribble', 'No', 'major', 'now', 'minstrels', 'whether', 'Ayy', 'bastards', 'having', 'yet', 'husk', 'Farewell', 'uhh', 'else', 'land', 'illegitimate-faced', 'tiny', 'worthy', 'wide', 'Right', 'tough', 'filth', 'approaching', 'CARTOON', 'worst', 'house', 'foe', 'Dappy', 'CART-MASTER', 'Together', 'join', 'Lake', 'necessary', '!', 'outdoors', 'life', 'committed', 's', 'Do', 'CROWD', 'Midget', 'twenty', 'bats', 'frozen', 'basis', 'zoosh', 'GIRLS', 'thirty-seven', 'air-speed', 'suspenseful', 'Well', 'was', 'ALL', 'tit', 'home', 'guided', 'duck', 'sire', 'cop', 'history', 'horse', 'say', 'Chicken', 'un', 'pram', 'mayhem', 'sword', 'supports', 'Um', 'Dramatically', 'Actually', 'discovers', 'At', 'fooling', 'questions', 'witch', 'Ah', 'knights', 'Welcome', '...', 'nick', 'am', 'Every', 'evil', 'settles', 'Maynard', 'FATHER', 'punishment', \"'round\", 'approacheth', 'being', 'sign', 'song', 'n', 'ones', 'rest', 'Good', 'With', 'changed', 'tiny-brained', 'mightiest', \"'To\", 'PARTY', 'Launcelot', 'velocity', 'look', 'Why', 'laden', 'buggered', 'There', 'climes', 'discovered', 'stops', 'farcical', 'Angnor', 'do', 'food', 'fire', 'OF', 'emperor', 'consulted', 'trusty', 'bang', 'seemed', 'Swamp', \"'forgive\", 'mine', 'open', 'None', 'Remove', 'Ewing', 'thou', \"'s\", 'grovel', 'Tale', 'bet', 'Bors', 'Hand', 'stayed', 'cover', 'Mercea', 'Away', 'ni', 'scott', 'boom', 'the-not-quite-so-brave-as-Sir-Lancelot', 'Hyy', 'bad', \"'d\", ']', 'year', 'cave', 'women', 'Neee-wom', 'sing', 'On', 'Aggh', 'bicker', 'voluntarily', 'pure', 'smashing', 'mangled', 'courage', 'something', 'internal', 'mayest', 'shrubber', 'Lead', 'donkey-bottom', 'imprisoned', 'NI', 'Where', 'His', 'nose', 'sometimes', 'Hm', 'there', 'demand', 'uuup', 'mooo', 'wounding', 'headoff', 'WOMAN', 'types', 'clank', 'miserable', 'dangerous', 'Aauuuves', 'Jesus', 'Will', 'fly', 'non-migratory', '14', 'Autumn', 'call', \"'S\", 'Yeaaah', 'arrange', 'thump', 'learning', 'repressed', 'throat', 'working', 'sequin', 'sir', 'Hic', '[', 'England', 'afraid', 'remember', 'let', 'wrong', 'sure', 'married', 'ran', 'already', 'domine', 'walk', 'me', 'carry', 'Arimathea', 'k-nnniggets', 'next', 'pack', 'Erm', 'intermission', 'Huh', 'grail', 'Bedevere', 'quite', 'twenty-four', 'somewhere', 'GOD', 'breath', 'should', 'ugly', 'Stop', 'use', 'ju', 'outwit', 'coconuts', 'pause', 'gurgle', 'formed', 'carries', 'de', 'least', 'If', 'Book', 'HISTORIAN', 'shimmering', 'every', 'Speak', 'nor', 'perpetuates', 'tropical', 'blessing', 'liver', 'LUCKY', 'ungallant', 'chu', 'The', 'sort', 'try', 'pound', 'Guards', 'maintain', 'giggle', 'returns', 'BEDEVERE', 'tackle', 'word', 'for', 'Dragon', 'feel', 'Eee', 'Camaaaaaargue', 'Cornwall', 'Umhm', 'impersonate', 'b', 'medieval', 'guards', 'mac', 'shivering', 'very', 'once', 'vouchsafed', 'dad', 'averting', 'indefatigable', 'whispering', 'awaaaaay', 'oooh', 'killer', 'MIDGET', 'feint', 'strand', 'moistened', 'other', 'question', 'shrubbery', 'enter', 'African', 'wound', 'way', 'persons', 'yours', 'passing', 'easily', 'meant', 'example', 'That', 'largest', 'Must', 'unhealthy', 'rescue', 'Honestly', 'Explain', 'PRINCESS', 'haaa', 'imperialist', 'How', 'Ages', \"'Dennis\", 'Anybody', 'wounded', 'only', 'Between', '15', 'nothing', 'table', 'enchanter', 'Does', 'CRONE', 'Victory', 'five', 'Behold', 'My', 'Attila', 'WINSTON', 'carp', 'workers', 'give-away', 'wood', 'he', 'Almighty', 'remembered', 'feet', 'aaaaaah', 'guest', 'must', 'Father', 'wart', 'or', 'over', 'Schools', \"'is\", 'merger', 'dungeon', 'distress', 'individually', 'spanked', 'sent', 'Aaauggh', 'given', 'crone', 'son', 'Tim', 'affairs', 'thud', 'alive', 'whop', 'Prince', 'Bristol', 'ooh', 'Beyond', 'it', \"'Ere\", 'chosen', 'Use', 'any', 'Hiyah', 'aunties', 'makes', 'back', 'young', 'eis', 'tree', 'crying', 'Beast', 'hospital', 'direction', 'Hold', 'fine', 'name', 'haste', 'spam', 'MIDDLE', 'carving', 'man', 'named', 'p', 'witches', 'relics', 'shit', 'strange', 'traveller', 'away', 'him', 'Torment', '17', 'So', 'government', 'Mud', 'first', 'earth', 'ANIMATOR', 'thy', 'nearly', 'chord', 'resting', 'differences', '--', 'met', 'taunt', 'Order', 'As', 'safety', '20', 'wipers', 'through', 'Uh', 'Brave', 'hear', 'allowed', 'wan', 'Castle', 'plover', 'forth', 'kick', 'ready', 'properly', 'groveling', 'so', 'couple', 'cheesy', 'cast', '24', 'might', 'dull', 'grail-shaped', 'breakfast', 'lord', 'mooooooo', 'two-level', '.', 'resumes', 'Bones', 'tinder', 'vary', 'taking', 'Steady', 'Saint', 'separate', 'bringing', 'servant', 'if', \"'ve\", 'Hello', 'long', 'shall', 'court', 'Camelot', 'tragic', 'tart', 'Anyway', 'miss', 'request', 'quick', 'Britons', 'seems', 'bottoms', 'fought', 'swallows', 'pissing', 'noise', 'everyone', 'become', 'tear', 'mystic', 'BROTHER', 'most', 'Aramaic', 'bless', 'dramatic', 'GREEN', 'marrying', 'could', 'awaaay', 'Assyria', 'Thppt', 'empty', 'trouble', 'uuggggggh', 'maybe', 'Lady', 'quests', 'Supposing', 'biscuits', \"'anging\", 'automatically', 'Augh', 'nightfall', 'continue', 'bleeder', 'lot', 'GUESTS', 'buy', 'under', 'convinced', 'unsingable', 'Quiet', 'reached', 'FRENCH', 'fled', 'Grenade', ':', 'Brother', 'take', 'side', 'suggesting', 'anyone', 'autocracy', 'pweeng', 'went', 'scimitar', 'such', 'Table', 'thought', 'lives', 'ehh', 'Nay', 'chops', 'a', 'Quite', 'CONCORDE', 'warmer', 'Forward', 'Throw', 'Excuse', 'creature', 'armor', 'Two', '9', 'Let', 'made', 'too', 'feast', 'idea', 'running', 'pig-dogs', 'Dennis', 'lambs', 'Yup', 'banana-shaped', 'new', 'listen', 'cruel', 'SUN', 'Mind', 'Britain', 'Then', 'harmless', 'since', 'strangers', 'cough', 'vests', 'Hiyaah', 'danger', 'does', 'Roger', 'Divine', 'looked', 'cereals', 'dying', 'moment', 'migrate', 'terrible', 'spoken', 'burst', 'Shrubber', 'sawwwww', 'inherent', 'mortally', 'animal', 'sample', 'they', \"'re\", '4', 'last', 'gave', 'looks', 'reads', 'band', 'Gorge', 'aaugh', '(', 'supreme', 'dead', 'absolutely', 'we', 'beds', 'Build', 'HERBERT', 'grip', 'Frank', 'hills', 'kills', 'nervous', 'seek', 'fifty', 'glass', 'Providence', 'Found', 'bowels', 'legally', 'accent', 'ruffians', 'excuse', 'pen', 'seen', 'third', 'hundred-and-fifty', \"'Erbert\", 'lads', 'leave', 'dressing', 'fatal', 'Uuh', '11', 'trade', 'MAN', 'Fine', 'k-niggets', 'runes', 'To', 'Robin', 'lobbest', 'MONKS', 'scales', 'Meanwhile', 'bonk', 'two', 'ordinary', 'himself', 'real', 'much', 'little', 'riding', 'hee', 'run', 'kneeling', 'Hiyya', 'Firstly', 'tell', 'smelt', 'doubt', 'She', 'left', 'telling', 'Bread', 'has', 'all', 'refuse', 'glad', 'fell', 'carried', 'Eternal', 'guiding', 'stand', 'reasonable', 'Our', 'strategy', 'Grail', 'completely', 'throwing', 'twin', 'Pendragon', 'Lucky', 'warning', 'Of', 'sponge', 'color', 'ham', 'of', 'but', 'outrageous', 'Idiom', 'tie', 'limbs', 'tail', 'bleed', \"'Ecky-ecky-ecky-ecky-pikang-zoop-boing-goodem-zoo-owli-zhiv\", 'three', 'i', 'Peril', 'Thpppt', 'er', 'underwear', 'liar', 'purpose', 'afoot', 'right', ')', 'heads', 'against', 'Round', 'Herbert', 'Apples', 'force', 'Prepare', 'the', \"'Aauuuuugh\", 'Peng', 'Exactly', 'assault', \"'\", 'bitching', 'sharp', 'my', \"d'you\", 'Saxons', 'dine', 'tea', 'beacon', 'talk', 'Antioch', 'problem', 'beside', 'Hill', 'Aauuugh', 'attend', 'fair', 'kingdom', 'hamster', 'room', 'spake', 'rich', 'DEAD', 'splat', 'wield', 'wo', '23', 'anywhere', 'clad', 'certainly', 'boil', 'o', 'binding', 'MAYNARD', 'body', 'flight', 'SIR', 'general', 'Go', 'scots', 'identical', 'again', 'bits', 'ethereal', 'SENTRY', 'trough', 'social', 'like', 'masses', 'large', 'starling', 'Five', 'need', 'Stand', 'Monsieur', 'Agh', 'model', 'became', 'Nine', 'guard', 'good', 'Aagh', '19', 'cry', 'gouged', 'Not-appearing-in-this-film', 'OFFICER', 'its', 'Huyah', 'Nador', 'special', 'suddenly', 'want', 'feathers', 'oo', 'Walk', 'many', 'rope', 'Bedwere', 'wave', 'thine', 'brain', 'oui', 'sixteen', 'own', 'Haw', 'Really', 'Run', 'fold', 'weather', 'leg', 'used', 'legs', 'bloody', 'dragging', 'Unfortunately', 'Aaaugh', 'living', 'in', 'Ninepence', 'parts', 'Knight', 'luck', 'bed', 'hoo', \"'m\", 'same', 'up', 'Blue', 'Iiiives', 'King', 'brush', 'Doctor', 'power', 'martin', \"'it\", 'Burn', 'illustrious', '13', 'hospitality', 'clop', 'weapon', 'people', 'indeed', 'delirious', 'walking', 'help', 'these', 'chanting', 'count', 'score', 'bridges', 'sloths', 'saw', 'Holy', 'Aaaaaaaah', 'really', 'slightly', 'plan', 'helpful', 'put', 'Chaste', 'yourself', 'hello', 'thwonk', 'Shrubberies', 'sneaking', 'mate', 'eet', 'scarper', 'guests', 'Badon', 'May', 'hacked', 'dona', 'bunny', 'unclog', 'preserving', 'kill', 'pulp', 'Perhaps', 'Clark', '..', 'person', 'Tall', 'twong', 'swamp', 'It', 'got', 'Oh', 'on', 'u', 'bois', 'apart', 'said', 'bastard', 'chastity', 'surprise', 'leaps', \"n't\", 'avenged', 'clue', 'whose', 'lie', 'nine', 'Loimbard', 'Piglet', 'knock', 'Oooo', 'Winter', 'length', 'although', 'burn', 'idiom', 'considerable', 'dictating', 'Crapper', 'training', 'Mine', 'Uther', 'died', 'forced', 'more', 'exploiting', 'happens', 'pay', 'bond', 'setting', 'night', 'needs', 'Christ', 'vicious', 'did', 'who', '16', 'Which', 'Thppppt', 'covered', 'point', 'aquatic', 'bosom', 'mean', 'favorite', 'keepers', 'valiant', 'yel', 'bangin', 'Mother', 'marry', 'broken', 'sigh', 'handsome', 'matter', 'Woa', 'door', 'wicked', 'oral', 'just', 'Tower', 'Did', 'stay', 'one', 'kind', 'PATSY', 'Back', 'Ow', 'always', 'spank', 'headed', 'DENNIS', 'bi-weekly', 'Black', 'Today', 'move', 'clllank', 'number', 'What', 'employed', 'commune', 'ceremony', 'wings', 'be', 'freedom', 'officer', 'Yay', 'Look', 'HEADS', 'false', 'done', 'halves', 'private', 'worried', 'Bon', 'Not', 'enemies', 'sister', 'bows', 'OLD', 'TIM', 'Ni', 'ninepence', 'offensive', 'Rather', 'kicked', 'knocked', 'ere', 'easy', 'quiet', 'warm', 'profane', 'knees-bent', 'dictatorship', 'Pure', 'Four', 'duty', 'Patsy', 'Whoa', 'wonderful', 'Once', 'worse', 'passed', 'eccentric', 'weight', 'cope', 'Ives', 'ignore', 'depressing', 'hand', 'present', 'Hmm', 'successful', 'elbows', 'time-a', 'j', 'argue', 'BRIDGEKEEPER', 'executive', 'particular', 'And', '21', 'string', '12', 'stew', 'sense', \"'Morning\", 'Seek', 'dancing', 'understanding', 'called', 'lunged', 'particularly', '#', 'an', 'angels', 'Over', 'elderberries', 'return', 'Gawain', '5', 'getting', 'suit', 'Looks', 'Until', 'castanets', 'were', 'supposed', 'sovereign', 'know', 'held', 'Waa', 'arrows', 'certain', 'inside', 'arms', 'yelling', 'heroic', 'newt', 'ounce', 'huge', 'round', 'bride', 'throughout', 'started', 'because', 'bridgekeeper', 'ill.', 'sod', 'Consult', 'king-a', 'Hurry', 'kings', 'anyway', 'sheep', 'felt', 'order', 'undressing', 'forget', 'change', 'Man', 'thing', 'blondes', 'strongest', 'bad-tempered', 'path', 'stuffed', 'words', 'Everything', 'around', 'wedding', 'um', 'says', 'y', 'CHARACTERS', 'Concorde', 'majority', 'Very', 'NARRATOR', 'build', '6', 'main', 'eh', 'aloft', 'Other', 'as', 'For', 'someone', 'confuse', 'jokes', 'until', 'ride', 'into', \"'shrubberies\", 'raised', 'pestilence', 'heh', 'totally', 'hiyaah', 'Armaments', 'Thpppppt', 'their', 'winter', 'recover', 'eyes', '8', 'Twenty-one', 'HEAD', 'lair', 'inferior', 'scenes', 'shows', 'baby', \"'aaggggh\", 'LOVELY', 'another', 'Heh', 'bathing', 'proceed', 'wayy', 'Auuuuuuuugh', 'Far', \"'ll\", 'that', 'taken', 'entering', 'class', 'INSPECTOR', 'period', 'Are', 'clunk', 'union', 'Off', 'stood', 'some', 'packing', '1', 'hidden', 'pussy', 'sight', 'Ohh', 'assist', 'times', 'donaeis', 'bid', 'Arthur', 'holy', 'Ector', 'ours', 'from', 'doctors', 'entrance', 'owns', 'mother', 'shelter', 'Bridge', 'see', 'Put', 'Lancelot', 'beautiful', 'come', 'U', 'violence', 'what', \"e'er\", 'SOLDIER', 'Help', 'will', 'she', 'acting', 'previous', 'nostrils', 'Please', 'Great', 'awaits', 'deal', 'Rheged', 'mile', 'KNIGHTS', 'making', 'seem', 'Chickennn', 'full', 'you', 'Thursday', 'dogma', 'utterly', 'folk', 'hall', 'By', 'Un', 'teeth', 'dress', 'so-called', 'stop', 'this', 'dark', 'along', 'N', \"'Ni\", 'accompanied', 'week', 'whoever', 'Heee', 'Hallo', 'actually', 'turns', 'derives', 'dub', 'ferocity', 'better', 'Aaagh', 'clack', 'Sir', 'Bravest', 'drilllll', 'near', 'mangy', 'foot', 'looney', 'gone', 'Death', 'names', 'hat', 'Dis-mount', 'legendary', \"'em\", 'Most', 'course', 'diaphragm', 'Hah', 'snows', 'collective', 'ridden', 'interested', 'give', 'chickened', 'retold', 'personally', 'remain', 'get', 'is', 'stretched', '22', 'fight', 'Silly', 'MINSTREL', 'removed', 'depart', 'Like', 'Those', 'girl', \"'Course\", 'behold', 'problems', 'sank', 'stress', 'dirty', 'na', 'using', 'effect', 'Fiends', 'purest', 'A', 'sacred', 'been', 'place', \"'old\", 'threw', 'decision', 'Oooohoohohooo', 'temptation', 'types-a', 'clang', 'dappy', 'slash', 'sweet', 'Allo', 'appease', 'invincible', 'Even', 'nice-a', 'woods', 'Picture', 'temptress', \"'cause\", 'welcome', 'Caerbannog', 'please', 'Zoot', 'performance', 'outside', 'day', 'charged', 'Halt', 'told', 'buggering', 'small', 'Battle', 'advancing', 'brave', 'amazes', 'ca', 'bring', 'can', 'lost', 'sun', 'entered', 'Here', 'travellers', 'forty-three', 'An', 'electric', 'high-pitched', 'design', 'Lord', 'snap', 'king', 'higher', 'gained', 'sonny', 'wooden', 'Winston', \"'sorry\", 'ladies', 'wet', 'siren', 'master', 'fwump', 'bridge', 'rrrr', 'KING', 'tonight', 'busy', 'town', 'expensive', 'joyful', 'Supreme', 'stab', 'awfully', 'Guy', 'Hey', 'yeah', 'still', 'Defeat', 'Lie', 'ye', 'yes', 'Be', 'great', 'therefore', 'earthquakes', 'tale', 'chickening', \"'aaaah\", 'expect', 'have', 'liege', 'reared', 'ENCHANTER', 'armed', 'distributing', 'Eh', 'Your', 'immediately', 'Old', 'Iesu', 'fallen', 'manner', 'agree', 'knows', 'clear', 'twang', 'scrape', 'stone', 'centuries', 'peril', 'late', 'beyond', '7', 'hmm', 'not', 'BLACK', 'apologise', 'his', 'grips', 'lad', 'send', 'snuff', 'later', 'asks', 'search', 'Himself', \"'Til\", 'lapin', 'shrubberies', 'dressed', \"'Man\", 'men', \"'til\", 'Huy', '?', 'medical', 'either', 'foul', 'suppose', 'also', 'dunno', 'finest', 'keen', '2', 'mad', 'Psalms', 'found', 'keep', 'somebody', 'enjoying', 'tired', 'Surely', 'ask', 'ROBIN', 'tracts', 'squeak', 'French', 'two-thirds', 'death', ',', 'Cider', 'PRISONER', 'taunting', 'Shall', 'Open', 'unplugged', 'OTHER', 'varletesses', 'ratios', 'sink', 'Shh', 'which', 'floats', 'Ha', 'DINGO', 'la', 'each', 'dorsal', 'More', 'handle', 'blanket', 'protect', 'Aauuggghhh', 'vache', 'purely', 'killed', 'almost', 'answers', 'today', 'hast', 'chorus', 'built', 'bold', 'test', 'peasant', 'W', 'When', 'Greetings', 'RIGHT', 'them', 'ROGER', 'I', 'saying', 'quest', 'AMAZING', 'deeds', 'SECOND', 'rabbit', 'Gable', 'Excalibur', 'to', 'heard', 'gay', 'Get', 'find', 'Robinson', 'cost', 'badger', 'scratch', 'single-handed', 'rodent', 'end', 'think', 'snore', 'when', 'and', 'thank', 'routines', 'time', 'flights', 'rock', 'influential', 'defeator', 'eight', 'coconut', 'sad', 'country', 'adversary', 'Wait', 'treat', 'blood', 'yellow', 'smack', 'lived', 'You', 'turned', 'jam', 'head', 'face', 'bother', 'war', 'tap-dancing', 'van', 'behind', 'O', 'never', 'chest', 'system', 'Running', 'anarcho-syndicalist', 'show', 'commands', 'flint', 'frontal', \"C'est\", 'forest', 'may', 'here', 'Alice', 'Yeah', 'splash', 'object', 'Too', 'bum', 'Aaah', 'suffice', 'Galahad', 'sons', 'longer', 'WITCH', 'draw', 'CHARACTER', 'VILLAGERS', 'then', 'eats', 'hell', 'Amen', 'mashed', 'old', 'defeat', 'Aah', 'our', 'wiper', 'pull', 'capital', 'Yeaah', 'Cut', 'Iiiiives', 'creeper', 'off', 'make', 'VOICE', 'baaaa', 'We', \"'Oooooooh\", 'science', 'lucky', 'business', 'presence', 'humble', 'middle', 'BORS', 'Yes', 'Nothing', 'kneecaps', 'They', 'plain', 'counting', 'Bravely', 'Knights', 'Stay', 'e', 'Aaauugh', 'Ridden', 'grin', 'Wood', 'smashed', 'formidable', 'north-east', 'Recently', 'Pin', \"'T\", 'escape', 'k-nnnnniggets', '3', 'Cherries', 'decided', 'howl', 'autonomous', 'Uugh', 'jump', 'hopeless', 'out-clever', 'Aaaah', 'magne', 'prevent'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "with open('scene_one.txt') as f:\n",
    "    scene_one = f.read()\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab82e976-c32e-429c-a3f5-6ab3f0eba2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16777e88-86da-4547-a2cc-7aa8d5e6fe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860f5f08-09da-49e2-90ac-f2022c9b11aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "sentences = sent_tokenize(scene_one)\n",
    "pattern2 = r\"[\\w+\\s+]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab11a97-b355-4365-bbd2-b49873efe885",
   "metadata": {},
   "source": [
    "## Advanced Tokenization with Regex\n",
    "### Regex using or \"|\"\n",
    "- OR is represented using the pipe character |\n",
    "- define a group using ()\n",
    "- you can define explicit character ranges using []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "585ad9c8-ed67-4e46-8223-a39aed28c73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "re.findall(match_digits_and_words, 'He has 11 cats.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f83d7fe-c48d-4924-98ee-e6dee098ccb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "from nltk import regexp_tokenize\n",
    "\n",
    "regexp_tokenize(my_string, r\"(\\w+|#\\d|\\?|!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1888f83-6dc5-46c2-8b3a-fe8f60f100ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtas:  ['#nlp', '#python']\n",
      "mentions/hashtags:  ['@datacamp', '#nlp', '#python']\n",
      "TweetTokenizer:  [['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    "          '#NLP is super fun! <3 #learning',\n",
    "          'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print('hashtas: ',hashtags)\n",
    "\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@|#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print('mentions/hashtags: ',mentions_hashtags)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print('TweetTokenizer: ',all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc8ab0-bd22-489a-aa37-d4908fd991e1",
   "metadata": {},
   "source": [
    "#### Non-ascii Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ded301d-9d43-4132-9e64-306496a1714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
      "['Wann', 'Pizza', 'Und', 'Über']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize, word_tokenize\n",
    "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971f6f2-52a0-48cc-8ac5-4052a0bae46a",
   "metadata": {},
   "source": [
    "## Charting word length with nltk\n",
    "#### Chart the length of each line in scene one by word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21c2372d-3b01-4d29-85f0-30c448a04e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split the script into lines: lines\n",
    "lines = scene_one.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, '\\w+') for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e982700-9955-4b1d-9406-0835f0092c1b",
   "metadata": {},
   "source": [
    "# Ch. 2 Simple Topic Identification\n",
    "## Word Counts with bag-of-words\n",
    "- bag-of-words is a simple and basic method for finding topics in a text sample\n",
    "- Need to first create tokens using tokenization and then count up all the tokens\n",
    "- The more frequent a word, the more important it might be to the theme of the text\n",
    "- bag-of-words can be a great way to determine the significant words in a text based on the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655469eb-e388-48e2-8871-29b95740803a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 2), ('cat', 2), ('box', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(word_tokenize('The cat is in the box. The cat box.'))\n",
    "counter.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d67c8-da5b-4c1c-99a4-da938fc31d70",
   "metadata": {},
   "source": [
    "### Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb60c3b5-db67-42e9-a9be-a63114c77ad6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "with open('wiki_text_debugging.txt') as f:\n",
    "    article = f.read()\n",
    "\n",
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c433a-d1ef-4bac-bc03-1f2f2b4e2e07",
   "metadata": {},
   "source": [
    "## Simple Text Preprocessing\n",
    "Why Preprocess?\n",
    "- Helps for better input data when performing machine learning or other statistical methods\n",
    "- examples\n",
    "    - tokenization to create a bag of words\n",
    "    - lowercasing words to avoid duplicates due to case\n",
    "- Lemmatization/Stemming: shortening words to their root or stem words\n",
    "- Removing stop words (and or the), punctuation, or unwanted tokens \n",
    "\n",
    "## Text Preprocessing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2bebff7-abba-4a60-a42d-7313178fb66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = 'The cat is in the box. The cat likes the box. The cat likes the box.'\n",
    "\n",
    "# Tokenize using a list comprehension\n",
    "# isalpha returns True if the token is alphabetical. Used to filter our numbers and punctuation\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "\n",
    "# Filter out stopwords\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "# Create counter\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87440e-9a62-41bd-8a29-80e781c41b21",
   "metadata": {},
   "source": [
    "### Text preprocessing practice\n",
    "Remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "343bd7c2-aaab-4ca4-ab02-39047359a6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Read in Text\n",
    "with open('wiki_text_debugging.txt') as f:\n",
    "    article = f.read()\n",
    "    \n",
    "# Tokenize in lowercase\n",
    "lower_tokens = [w for w in word_tokenize(article.lower())]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae967d-d5d6-406a-a343-b3e57bc6741e",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "- Popular open-source NLP library\n",
    "- Uses top academic models to perform complex tasks\n",
    "    - building documents or word vectors\n",
    "    - performing topic identification and document comparison\n",
    "\n",
    "### Creating and querying a corpus with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8d8caff-5346-4aee-a193-f65636b8ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# create a list of files to parse\n",
    "wiki_titles = ['wiki_text_bug.txt', 'wiki_text_computer.txt', 'wiki_text_crash.txt', 'wiki_text_debugger.txt',\n",
    "               'wiki_text_debugging.txt', 'wiki_text_exception.txt', 'wiki_text_hopper.txt', 'wiki_text_language.txt', \n",
    "               'wiki_text_malware.txt', 'wiki_text_program.txt', 'wiki_text_reversing.txt', 'wiki_text_software.txt', ]\n",
    "\n",
    "# Create list to store document tokens in\n",
    "articles = []\n",
    "\n",
    "# Loop through articles and store tokens as a list in articles\n",
    "for wt in wiki_titles:\n",
    "    # Read in Text\n",
    "    with open(wt, encoding='UTF-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Tokenize in lowercase\n",
    "    lower_tokens = [w for w in word_tokenize(text.lower())]\n",
    "\n",
    "    # Retain alphabetic words: alpha_only\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "    # Remove all stop words: no_stops\n",
    "    no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "    \n",
    "    # Add preprocessed tokens to articles\n",
    "    articles.append(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cba9897-7b8d-4e9c-aa81-fc3d5a6f0d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "[(1, 1), (13, 1), (15, 1), (18, 1), (26, 1), (29, 1), (37, 1), (38, 4), (47, 2), (48, 7)]\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "711bbd71-3e7d-4f15-ae12-0d04948d8043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 for doc:\n",
      "bug 66\n",
      "bugs 60\n",
      "software 55\n",
      "may 45\n",
      "computer 38\n",
      "\n",
      "Top 5 overall:\n",
      "computer 598\n",
      "software 450\n",
      "cite 322\n",
      "ref 259\n",
      "code 235\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[0]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "print('Top 5 for doc:')\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "print('\\nTop 5 overall:')\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd744f7-f8d0-4cec-b8b7-4a898200c41d",
   "metadata": {},
   "source": [
    "## Tf-idf with gensim\n",
    "Term frequency - inverse document frequency\n",
    "- Allows you to determine the most important words in each document\n",
    "- Each corpus may have shared words beyond common stopwords\n",
    "- These words should be down-weighted in importance \n",
    "- Ensures the most common words accross the corpus don't show up as keywords\n",
    "- Keeps document specific words weighted high, while common words across the whole corpus are weighted low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "756731a6-6d07-4e73-8c41-bd603d02a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priority 0.3415630199292239\n",
      "severity 0.29886764243807085\n",
      "releases 0.19212919871018844\n",
      "bug 0.1631163587170805\n",
      "bugs 0.14828759883370954\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Assign a corpus to doc\n",
    "doc = corpus[0]\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205a620-1eeb-43f0-a16d-e8e2e0b11a6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ch. 3 Name Entity Recognition (NER)\n",
    "- NLP task to identify important named entities in the text\n",
    "- people, places, organizations\n",
    "- Dates, states, works of art\n",
    "- and many other categories depending on the library and notation used\n",
    "- Can be used alongside topic identification\n",
    "- or on its own to answer questions such as who, what, when, and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74dc1167-a285-4e3c-ac72-fef71bf6af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Uber/NNP)\n",
      "(NE Beyond/NN)\n",
      "(NE Apple/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Travis/NNP Kalanick/NNP)\n",
      "(NE Tim/NNP Cook/NNP)\n",
      "(NE Apple/NNP)\n",
      "(NE Silicon/NNP Valley/NNP)\n",
      "(NE CEO/NNP)\n",
      "(NE Yahoo/NNP)\n",
      "(NE Marissa/NNP Mayer/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "with open('uber_apple.txt', encoding='UTF-8') as f:\n",
    "    article = f.read()\n",
    "    \n",
    "# Tokenize the article into sentences: sentences\n",
    "sentences = sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21f180c1-3a28-47bd-a252-7d55d0aafd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "with open('articles.txt', encoding='UTF-8') as f:\n",
    "    articles = f.read()\n",
    "\n",
    "# Tokenize the article into sentences: sentences\n",
    "sentences = sent_tokenize(articles)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b976fa3-d8aa-43f1-8fa7-80840671456e",
   "metadata": {},
   "source": [
    "## Introduction to SpaCy\n",
    "- NLP Library similar to gensim, but with different implementations\n",
    "- focus on creating NLP pipelines to generate models and corpora\n",
    "- open source with extra libraries and tools\n",
    "    - Displacy: visualization tool for viewing parse trees which uses nodejs to create interactive text\n",
    "\n",
    "### Why use SpaCy for NER?\n",
    "- Easy pipeline creation\n",
    "- Different Entity types compared to nltk\n",
    "- informal language corpora\n",
    "    - easily find entites in Tweets and chat messages\n",
    "- Quickly growing and having new languages/tools added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9d2894e-a194-4bfe-9e78-fbbee27dc685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG unroll.me\n",
      "ORG Apple\n",
      "PERSON Uber\n",
      "FAC Travis Kalanick of Uber\n",
      "PERSON Tim Cook\n",
      "ORG Apple\n",
      "CARDINAL Millions\n",
      "PERSON Uber\n",
      "LOC Silicon Valley\n",
      "ORG Yahoo\n",
      "PERSON Marissa Mayer\n",
      "MONEY 186\n"
     ]
    }
   ],
   "source": [
    "with open('uber_apple.txt', encoding='UTF-8') as f:\n",
    "    article = f.read()\n",
    "\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_sm',tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbe9c7-8d6c-4ebd-8ba1-9eaf07ffe90b",
   "metadata": {},
   "source": [
    "## Multilingual NER with polyglot\n",
    "- NLP that uses word vectors\n",
    "- Why polyglot?\n",
    "    - vectors for many languages\n",
    "    - more than 130 languages\n",
    "    - can use for transliteration/translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4dfe94-ddc6-4f8b-81c4-80ca98f85217",
   "metadata": {},
   "source": [
    "# Ch. 4 Building a \"Fake News\" Classfier\n",
    "## Supervised Learning with NLP\n",
    "- need to use language to classify observations\n",
    "- how to create supervised learning data from text?\n",
    "    - bag of words \n",
    "    - tf-idf \n",
    "\n",
    "Supervised Learning Process\n",
    "- collect and preprocess data\n",
    "- determine a label\n",
    "- split data into training and testing\n",
    "- extract features from text to predict the label\n",
    "- evaluate model using test data\n",
    "\n",
    "### Building word count vectors with scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7247df89-1851-4ad4-a676-1e98e0d3d05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('fake_or_real_news.csv', encoding='UTF-8')\n",
    "\n",
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a series to store the labels (y) and text (X)\n",
    "y = df['label']\n",
    "X = df['text']\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8e35e79-1ef8-440b-b60a-0974fa51012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "950f95c2-8f76-4d1a-9220-6be7279ad865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "\n",
      "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "\n",
      "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652f9eb-5bca-48ff-9112-49e71e24980e",
   "metadata": {},
   "source": [
    "### Training and testing with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83f0b52c-c040-4ac0-8cd7-805f057b880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893352462936394\n",
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc38a4-9868-4261-a611-e752179527fc",
   "metadata": {},
   "source": [
    "### Training and testing with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "755e4643-c0eb-4621-bd96-3e79ccc099fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62db14-fd96-4ec7-a879-f3e25626437a",
   "metadata": {},
   "source": [
    "## Simple NLP, complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97b59e1a-e0d0-4185-b120-6cc963996b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8813964610234337\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8589191774270684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16084\\anaconda3\\envs\\python_ML\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1f41893-ab08-4760-8f68-f0596dba04eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
      "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16084\\anaconda3\\envs\\python_ML\\lib\\site-packages\\sklearn\\utils\\deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d913da-e63c-440b-9e16-fc79d50f5fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
