{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec31766-6fb5-4cb2-a353-254581760045",
   "metadata": {},
   "source": [
    "# Ch. 1 Recurrent Neural Networks and Keras\n",
    "- Applications of machine learning to text data\n",
    "    - Sentiment Analysis\n",
    "        - classifying customer feedback into positive or negative to guage how the customer base feels on the business\n",
    "    - Multi-Class Classification\n",
    "        - Recommender systems. \n",
    "    - Text Generation\n",
    "        - Auto-complete sentences\n",
    "    - Machine Neural Translation\n",
    "        - Translate languages\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "- main advantage is that they reduce the number of parameters of the model by avoiding one-hot encoding\n",
    "- Sequence to Sequence Models\n",
    "    - Many to one: classification tasks\n",
    "        - final output is a probability distribution. Y-pred is the probability of the sentiment belonging to the class \"positive\"\n",
    "        - used on sentiment analysis and multi-classification applications\n",
    "    - Many to Many: text generation\n",
    "    - Many to Many: neural machine translation\n",
    "        - encoder block\n",
    "        - decoder block\n",
    "        \n",
    "## Introduction to Language Models\n",
    "### Sentence Probability\n",
    "- Many Available Models\n",
    "    - Probability of \"I loved this movie\"\n",
    "    - Neural Networks\n",
    "        - the probability of the sentence is given by a softmax function on the output layer of the network\n",
    "\n",
    "### Link to RNNs\n",
    "- Language models are everywhere in RNNs\n",
    "- the network itself is a language model when fed text data\n",
    "    - Give the probability of the next token given the previous tokens\n",
    "- Embedding layer can be used to create vector representations of the tokens in the first layer\n",
    "- Need to build vocabulary dictionaries where each unique work is assigned its number as it's index in the unique vocabulary array\n",
    "\n",
    "#### Building vocabulary dictionaries\n",
    "- get unique words/tokens from the corpus"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11cbad9b-bef6-45b0-806c-4429d1aa4d17",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "with open('text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Get Unique Words\n",
    "unique_words = list(set(text.split(' ')))\n",
    "\n",
    "# Create dictionary: work is key, index is value\n",
    "word_to_index = {k:v for (v,k) in enumerate(unique_words)}\n",
    "\n",
    "# Create dictionary: work is key, index is value\n",
    "index_to_word = {k:v for (k,v) in enumerate(unique_words)}\n",
    "\n",
    "# Initialize x and y\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop over the text: length 'sentence_size' per time with step equal to 'step'\n",
    "for i in range(0, len(text) - sentence_size, step):\n",
    "    X.append(text[i:i+ sentence_size])\n",
    "    y.append(text[i+sentence_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78504111-2711-4c5c-9902-4ac18ac3f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheldon_quotes = [\"You're afraid of insects and women, Ladybugs must render you catatonic.\",\n",
    "                 'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.',\n",
    "                 'For example, I cry because others are stupid, and that makes me sad.',\n",
    "                 \"I'm not insane, my mother had me tested.\",\n",
    "                 'Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.',\n",
    "                 \"Amy's birthday present will be my genitals.\",\n",
    "                 '(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!',\n",
    "                 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.',\n",
    "                 'I would have been here sooner but the bus kept stopping for other people to get on it.',\n",
    "                 'Oh gravity, thou art a heartless bitch.',\n",
    "                 'I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.',\n",
    "                 'Well, today we tried masturbating for money.',\n",
    "                 'I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.',\n",
    "                 \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\",\n",
    "                 \"What computer do you have? And please don't say a white one.\",\n",
    "                 \"She calls me moon-pie because I'm nummy-nummy and she could just eat me up.\",\n",
    "                 'Ah, memory impairment; the free prize at the bottom of every vodka bottle.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ee969e-8013-4a16-a68c-bc4cf72d8a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(3': 0, 'Ah,': 1, \"Amy's\": 2, 'And': 3, 'Explorer': 4, 'Firefox.': 5, 'For': 6, 'Galileo,': 7, 'Goblin': 8, 'Green': 9, 'Hubble': 10, 'I': 11, \"I'm\": 12, 'Internet': 13, 'Ladybugs': 14, 'Oh': 15, 'Paul': 16, 'Penny': 17, 'Penny!': 18, 'Pope': 19, 'Scissors': 20, 'She': 21, 'Spider-Man,': 22, 'Spock': 23, 'Spock,': 24, 'Thankfully': 25, 'The': 26, 'Two': 27, 'V': 28, 'Well,': 29, 'What': 30, 'Wheaton!': 31, 'Wil': 32, \"You're\": 33, 'a': 34, 'afraid': 35, 'all': 36, 'always': 37, 'am': 38, 'and': 39, 'appeals': 40, 'are': 41, 'art': 42, 'as': 43, 'at': 44, 'aware': 45, 'based': 46, 'be': 47, 'became': 48, 'because': 49, 'been': 50, 'birthday': 51, 'bitch.': 52, 'black': 53, 'blood': 54, 'bottle.': 55, 'bottom': 56, 'brain': 57, 'breaker.': 58, 'bus': 59, 'but': 60, 'calls': 61, 'can': 62, 'care': 63, 'catatonic.': 64, 'center': 65, 'chance': 66, 'circuit': 67, 'computer': 68, 'could': 69, 'covers': 70, 'crushes': 71, 'cry': 72, 'cuts': 73, 'days': 74, 'decapitates': 75, 'deity.': 76, 'discovering': 77, 'disproves': 78, 'do': 79, 'does': 80, \"don't\": 81, 'eat': 82, 'eats': 83, 'every': 84, 'example,': 85, 'flashlight': 86, 'for': 87, 'free': 88, 'genitals,': 89, 'genitals.': 90, 'get': 91, 'ghost': 92, 'girlfriend': 93, 'gravity,': 94, 'had': 95, 'hand.': 96, 'has,': 97, 'have': 98, 'have?': 99, 'having': 100, 'heartless': 101, 'here': 102, 'hole': 103, 'humans': 104, 'if': 105, 'impairment;': 106, 'in': 107, 'insane,': 108, 'insects': 109, 'involves': 110, 'is': 111, \"isn't\": 112, 'it': 113, 'it.': 114, 'just': 115, 'kept': 116, 'knocks)': 117, 'later,': 118, 'little': 119, 'living': 120, 'lizard': 121, 'lizard,': 122, 'loud': 123, 'makes': 124, 'man': 125, 'masturbating': 126, 'me': 127, 'memory': 128, 'messy,': 129, 'money.': 130, 'moon-pie': 131, 'mother': 132, 'moved': 133, 'much': 134, 'must': 135, 'my': 136, 'next': 137, 'not': 138, 'nummy-nummy': 139, 'of': 140, 'on': 141, 'one.': 142, 'other': 143, 'others': 144, 'paper': 145, 'paper,': 146, 'people': 147, 'please': 148, 'poisons': 149, 'present': 150, 'prize': 151, 'relationship': 152, 'render': 153, 'reproduce': 154, 'right': 155, 'rock': 156, 'rock,': 157, 'rushed': 158, 'sad.': 159, 'say': 160, 'scissors': 161, 'scissors,': 162, 'scissors.': 163, 'searching': 164, 'sexual': 165, 'she': 166, 'smashes': 167, 'so': 168, 'sooner': 169, 'stopping': 170, 'stupid,': 171, 'taken': 172, 'telescope': 173, 'tested.': 174, 'that': 175, 'the': 176, 'things': 177, 'think': 178, 'thou': 179, 'three': 180, 'to': 181, 'today': 182, 'town.': 183, 'tried': 184, 'unnecessary': 185, 'unsanitary': 186, 'up.': 187, 'used': 188, 'usually': 189, 'vaporizes': 190, 'vodka': 191, 'way': 192, 'we': 193, 'well,': 194, 'which': 195, 'white': 196, 'will': 197, 'with': 198, 'women,': 199, 'would': 200, 'years,': 201, 'you': 202, 'your': 203}\n"
     ]
    }
   ],
   "source": [
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20add110-e43f-4e4f-8752-8f14df766775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2          # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i+chars_window])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c5eed-9ed2-4592-8344-1ffad141405f",
   "metadata": {},
   "source": [
    "#### Transforming new text\n",
    "In this exercise, you will transform a new text into sequences of numerical indexes on the dictionaries created before.\n",
    "\n",
    "This is useful when you already have a trained model and want to apply it on a new dataset. The preprocessing steps done on the training data should also be applied to the new text, so the model can make predictions/classifications.\n",
    "\n",
    "## Introduction to Keras\n",
    "\n",
    "- Keras: high level API built on top of tensorflow\n",
    "    - keras.models.\n",
    "        - Sequential: layers in model are run one after another. Easier\n",
    "        - Models: More flexibility with layers. can have multiple inputs and outputs\n",
    "    - keras.layers.\n",
    "        - Dense\n",
    "        - LSTM\n",
    "        - GRU\n",
    "        - Embedding\n",
    "        - Dropout\n",
    "        - Bidirectional\n",
    "    - keras.preprocessing\n",
    "        - sequence.pad_sequences(texts, maxlen=int)\n",
    "    - keras.datasets\n",
    "        - IMDB Movie reviews: sentiment analysis\n",
    "        - Reuters Newswire: multiclass classification with 4-6 classes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b5259",
   "metadata": {},
   "source": [
    "### Model Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0c1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"modelclass_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, 10)]        0         \n",
      "_________________________________________________________________\n",
      "LSTM (Dense)                 (None, None, 128)         1408      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, None, 1)           129       \n",
      "=================================================================\n",
      "Total params: 1,537\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# Define the input layer\n",
    "main_input = Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "dense_layer = Dense(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(dense_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e0957",
   "metadata": {},
   "source": [
    "### Sequential Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3311498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense (Dense)                (None, None, 128)         1408      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, None, 1)           129       \n",
      "=================================================================\n",
      "Total params: 1,537\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# Instantiate the class\n",
    "model = Sequential(name=\"sequential_model\")\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(Dense(128, input_shape=(None, 10), name=\"Dense\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181324a0",
   "metadata": {},
   "source": [
    "## Keras Preprocessing\n",
    "The second most important module of Keras is keras.preprocessing. You will see how to use the most important modules and functions to prepare raw data to the correct input shape. Keras provides functionalities that substitute the dictionary approach you learned before.\n",
    "\n",
    "You will use the module keras.preprocessing.text.Tokenizer to create a dictionary of words using the method .fit_on_texts() and change the texts into numerical ids representing the index of each word on the dictionary using the method .texts_to_sequences().\n",
    "\n",
    "Then, use the function .pad_sequences() from keras.preprocessing.sequence to make all the sequences have the same size (necessary for the model) by adding zeros on the small texts and cutting the big ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70373ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the sample texts: (54, 78)\n",
      "Now the texts have fixed length: 60. Let's see the first one: \n",
      "[ 0  0  0  0  0  0 24  4  1 25 13 26  5  1 14  3 27  6 28  2  7 29 30 13\n",
      " 15  2  8 16 17  5 18  6  4  9 31  2  8 32  4  9 15 33  9 34 35 14 36 37\n",
      "  2 38 39 40  2  8 16 41 42  5 18  6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "texts = np.array(['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however,                     if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
    "                     'Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of                             radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own                             discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?'])\n",
    "\n",
    "# Import relevant classes/functions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12dd95",
   "metadata": {},
   "source": [
    "# Ch. 2 RNN Architecture\n",
    "## Vanishing and Exploding Gradients\n",
    "https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11\n",
    "\n",
    "- Exploding Gradient: when working back propogation the derivatives multiply exponentially and eventually \"explode\" to infinity\n",
    "- Vanishing Gradient: when the gradients vanish, or go to zero. \n",
    "    - This is a much harder problem to solve because it is not as easy to detect. \n",
    "    - If the loss function does not improve on every step, is it because the gradients went to zero and thus didn't update the weights? Or is it because the model is not able to learn?\n",
    "    - This problem occurs more often in RNN models when long memory is required, meaning having long sentences\n",
    "\n",
    "## GRU and LSTM Cells\n",
    "Achieve good results in language modeling and solve the vanishing gradient problem\n",
    "\n",
    "### No more vanishing gradients\n",
    "- The simpleRNN cell can have gradient problems\n",
    "    - the weight matrix power t multiplies the other terms\n",
    "- GRU and LSTM cells don't have vanishing gradient problems:\n",
    "    - because of their gates\n",
    "    - Don't have the weight matrices terms multiplying the rest \n",
    "    - Exploding gradient problems are easier to solve\n",
    "\n",
    "### Usage in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7048944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Layers\n",
    "from keras.layers import GRU, LSTM\n",
    "\n",
    "# Instantiate a model\n",
    "model = Sequential()\n",
    "\n",
    "# Add Layers: units is number of memory cells to keep track of. return_sequences states whether the cell is going to be an input to another layer\n",
    "model.add(GRU(units=128, return_sequences=True, name='GRU Layer'))\n",
    "model.add(LSTM(units=64, return_sequences=False, name='LSTM Layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d834d3",
   "metadata": {},
   "source": [
    "## The Embedding layer\n",
    "- Advantages: \n",
    "    - Reduces dimensions needed for data\n",
    "    - Dense Representation\n",
    "    - Transfer Learning\n",
    "- Disadvantages:\n",
    "    - need to train lots of parameters to Learn\n",
    "    - Can make training slower\n",
    "- Should be the first layer of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "832f0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras import Sequential\n",
    "\n",
    "# Use embedding as first layer\n",
    "model.add(Embedding(input_dim=10000, # size of the vocabulary\n",
    "                    output_dim=300, # output of the embedding space\n",
    "                    trainable=True, # to update or not update this layers weights during training\n",
    "                    embeddings_initializer=None, # Transfer learning with pre-traiing weights\n",
    "                    input_length=120)) # Size of sequences, assumes the inputs have been padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc3a18",
   "metadata": {},
   "source": [
    "## Improving RNN Models \n",
    "- To improve the model's performance we can:\n",
    "    - Add the embedding layer\n",
    "    - increase the number of layers\n",
    "    - tune the parameters\n",
    "    - Increase vocabulary size\n",
    "    - accept longer sentences with more memory cells\n",
    "    \n",
    "- to avoid overfitting:\n",
    "    - Test using different batch sizes\n",
    "    - add Dropout layer\n",
    "    - Add dropout and recurrent_dropout parameters on RNN layers\n",
    "        - dropout: 'rate' parameter removes the specified percentage of input data to add noise to model\n",
    "        - 'recurrent_dropout' removes the specified percentage of input and memory cells respectively\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588430f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
